[ { "title": "Predicting Customer Purchases: A Beginner's Guide to E-Commerce Data Science", "url": "/posts/Predicting-Customer-Purchases-Beginners-Guide/", "categories": "python, data-science, ai, machine-learning", "tags": "python, data-science, ai, machine-learning", "date": "2025-04-02 00:00:00 +0200", "snippet": "Have you ever wondered how online stores seem to know exactly what you want to buy? Or how they manage to show you products you’re likely interested in? The secret lies in a field called predictive analytics, which is basically using past data to make educated guesses about the future.In this blog post, I’ll walk you through a real-world example of how data science helps an online store understand its customers better. Don’t worry if you’re new to this – I’ll break everything down into simple terms!The Business Problem: An Online Store in TroubleImagine an online store that’s facing some serious problems: Their website keeps crashing randomly Customers are getting frustrated with service interruptions They’re spending a lot of money fixing unexpected issuesThe company wants to use data to solve these problems. Specifically, they want to understand what makes customers buy products on their website, so they can improve the customer experience and reduce technical problems.The Data: Digital FootprintsThe company has collected information about 12,330 visits to their website. For each visit, they recorded things like: How many pages each visitor looked at How much time they spent on different types of pages Whether they came from Google, social media, or directly typed the website address Whether they were a new or returning visitor Whether they ended up buying something (this is what we want to predict!)This data is like digital footprints that customers leave behind as they browse the website.Exploring the Data: Looking for PatternsBefore jumping into complicated algorithms, I first wanted to understand what the data actually tells us. Here are some interesting things I discovered: Only about 15% of visitors actually make a purchase. This is normal for e-commerce but means we need to be careful about how we evaluate our predictions. There’s a strong relationship between something called “Page Value” and whether someone makes a purchase. Page Value is essentially how valuable a page is in leading to a purchase. Returning visitors are more likely to make purchases than new visitors. This makes sense people who come back probably already trust the site. Weekend shoppers behave differently than weekday shoppers. Some months have higher purchase rates than others – November and December show higher conversion rates, likely due to holiday shopping.These initial findings already give us valuable insights for the business!Building Predictive Models: Teaching Computers to Make GuessesNow comes the fun part – creating models that can predict whether a visitor will make a purchase. I tried four different approaches: Logistic Regression: A simple approach that’s easy to understand Decision Tree: Think of this as a flowchart of yes/no questions Random Forest: Imagine combining hundreds of decision trees and letting them vote XGBoost: A more advanced technique that often performs very wellFor each model, I followed these steps: Split the data: I used 70% of the data to train the models and kept 30% separate to test how well they perform. Prepare the data: Computers don’t understand things like “returning visitor” or “weekday” directly, so I converted these categories into numbers they can work with. Train the models: This is where the computer learns patterns from the data. Fine-tune the models: I tested different settings for each model to find what works best (this is called hyperparameter tuning). Evaluate performance: I checked how accurately each model could predict purchases. Results: Which Model Worked Best?After training all four models, I compared their performance:The Random Forest model performed best, correctly predicting purchases about 90.5% of the time. More importantly, it achieved what’s called an AUC score of 0.94, which is excellent! (AUC measures how well the model distinguishes between customers who will purchase and those who won’t.)I also looked at what factors were most important for predicting purchases:The top predictors were: Page Value (how valuable pages visited are toward conversion) Exit Rate (percentage of visitors who leave from specific pages) Time spent looking at product pages Bounce Rate (people who leave immediately) Time spent on administrative pages (like account settings, shipping info)Making Business Sense of the ResultsSo what does all this mean for our struggling e-commerce company? Here’s what they should do: Focus on product pages: Since time spent on product pages strongly influences purchases, they should make these pages load faster, look better, and contain more useful information. Pay attention to high-exit pages: Pages with high exit rates might have usability problems that need fixing. Create a real-time prediction system: They could use our model to identify visitors who are likely to make a purchase and offer them special deals or better customer service. Develop targeted marketing: Different strategies should be used for returning visitors versus new visitors. Monitor for technical problems: Pages with unusually high bounce rates might indicate technical problems that need fixing. Technical Details Made Simple: How the Models Actually WorkIf you’re curious about how these models actually make predictions, here’s a simplified explanation:Logistic Regression looks at all the factors (like time on site, visitor type, etc.) and assigns weights to each one. It then combines these weighted factors to calculate the probability of a purchase.Decision Trees work by asking a series of yes/no questions. For example: “Is the Page Value greater than 10?” → “Did they visit more than 5 product pages?” → and so on.Random Forest builds hundreds of different decision trees and lets them vote on the final prediction. This is more powerful because it combines many perspectives.XGBoost builds trees one after another, with each new tree focusing on fixing the mistakes made by previous trees.All models were evaluated on multiple metrics. One helpful visualization is the “Confusion Matrix” which shows correct and incorrect predictions:Why This Matters (Even If You’re Not a Data Scientist)Even if you never build a predictive model yourself, understanding this process helps you: Appreciate the power of data in making business decisions Ask the right questions when someone presents data-driven recommendations Understand the limitations of predictive models (they’re powerful but not perfect)The next time you’re shopping online and see a “Recommended for you” section, you’ll have a better idea of the complex analytics happening behind the scenes!Referenceshttps://medium.com/analytics-vidhya/predictive-web-analytics-a-case-study-f30feda45002https://archive.ics.uci.edu/dataset/468/online+shoppers+purchasing+intention+dataset" }, { "title": "Custom Kubernetes Operators in Golang with OperatorSDK and Kubebuilder", "url": "/posts/Custom-Operator-With-Operator-SDK/", "categories": "devops, sre, dev", "tags": "kubernetes, go, operatorsdk, gke, bdd, ginkgo, gomega, kubebuilder, operator-sdk", "date": "2023-01-06 00:00:00 +0100", "snippet": "Kubernetes OperatorsKubernetes Operators are patterns that help us extend the behavior of the cluster.Operators enable us to view an application deployed on Kubernetes as one item. That is your application can composed of a Pod, a Service, a ConfigMap a Deployement etcbut you get to manage it as one item and have a much better control of their lifecycle. The lifecycle includes but not limited to installation and configuration and also manage failover and recoveryrelying on the APIs and Patterns provided by Kubernetes.If you’re not new to Kubernetes you probably have used a couple of operators, like Grafana, Prometheus,and a couple of ones I blogged about in the past like Strimzi for deployment and management of Kafka Cass Operator for doing the same with Cassandra or DSE.Now that we know what operators are, how do we build one?.There are a couple of ways to build an operator. However, we will be focusing onbuild a simple one with Operator SDK. This is part of the Operator framework that is set of developer tools and Kubernetes components, that aid in Operator developmentand central management on a multi-tenant cluster.There are three options for Operator development with Operator SDK, that is Golang, Ansible, or Helm. This post is focused on doing this in Go.To get started, we first need to install the OperatorSDK.Kubebuilder is a framework for building Kubernetes Operators. Operator SDK uses Kubebuilder under the hood to do so for Go projects.Putting it all togetherPutting it all together we can build a Mock Operator. It won’t do much but we will get to use the Operator SDK to build a custom Operator that basically launchesa Kubernetes Deployment and helps us manage the lifecycle. This is available on Github as https://github.com/malike/mock-operator.Before we start building we can define our Mock Operator specification.i. Operator SpecificationapiVersion: app.malike.kendeh.com/v1alpha1kind: SampleKindmetadata: name: mock-samplespec: image: repository: ghcr.io/malike/mock-operator/sample-mock-service tag: latest pullPolicy: Always pullSecretName: - name: regcred nodes: 2 containerPort: 80 servicePort: 80Things to note:i. apiVersion of the Operator is app.malike.kendeh.com/v1alpha1 ii. The Operator has one kind which is SampleKind iii. The Operator basically deploys a custom image which will passed as image.repository iv. Other parameters that describe the image are in the image: {} block. v. We use nodes to specify the number of instances we want to deploy. iv. Configure the port for services and the pod as servicePort and containerPort respectively.After defining the specification we can proceed to the next step of actually buildingii. Start codingOnce you have Operator SDK installed, we can generate project files for our Mock Operator.By running this command, we create an initial package. operator-sdk init --domain malike.kendeh.com --repo github.com/malike/mock-operatorAfter generating the project we can proceed and generate the api controller. We want our API controller to be called SampleKind with group as app. operator-sdk create api --group app --version v1alpha1 --kind SampleKind --resource --controlleriv. Building OperatorBefore we start coding let us look at the structure of the source generated by operator-sdk using the two commands.This link on Operator SDK helps us understand the project layout structure much better.Our changes will be much focused on the api and the controllers folders.Base on our specification we can update the API spec of the operator to meet what defined.Our API has for main parameters, that is image: {}, node: 2, containerPort: 80, servicePort: 80. We can update the specication to include these parametes// SampleKindSpec defines the desired state of SampleKindtype SampleKindSpec struct {\t//+kubebuilder:validation:Type:=object\t// Image defines image configuration\tImage ImageSpec `json:\"image,omitempty\"`\t//+kubebuilder:validation:Type:=number\t//+kubebuilder:default:=2\t// Nodes defines number of instance\tNodes int32 `json:\"nodes,omitempty\"`\t//+kubebuilder:validation:Type:=number\t//+kubebuilder:default:=80\t// ContainerPort defines port for container\tContainerPort int32 `json:\"containerPort,omitempty\"`\t//+kubebuilder:validation:Type:=number\t//+kubebuilder:default:=80\t// ServicePort defines port for service\tServicePort int32 `json:\"servicePort,omitempty\"`}Using kubebuilder CRD marker validation, we can enforce rules for these parameters. For examplenodes should always be an int32.Once defined we can run the command make generate manifests and then using the helper classes set up by Operator SDK the CRD and codes containing DeepCopy, DeepCopyInto, and DeepCopyObject method implementations that basicallytransforms the YAML to objects usable in Go.After defining our API we can move to the controller package. There we should see the controller for the Kind generated by Operator SDK, called samplekind_controller.It has a reconcile function which is responsible for enforcing the desired state of the system based on the CR applied. So if we need certain changes appliedbased on the CR applied, we can write code for that in this section.The logic is here is pretty simple for the reconciliation function. Confirm if resource needs to be created If resource not found but should have existed, create it If resource exists, confirm if it is the same as specified in CRD If resource not found and it is not supposed to be created do nothing.Pretty simple logic. Remember this function will be called in cycles.For our MockOperator, we will need a two k8s resources. A Deployment Service to expose our deploymentPutting this together we will have something like this in our reconcile function:// Fetch the SampleKind instance\tsampleApp := &amp;appv1alpha1.SampleKind{}\terr := r.Get(ctx, req.NamespacedName, sampleApp)\tif err != nil {\t\tif errors.IsNotFound(err) {\t\t\t// Request object not found, could have been deleted after reconcile request.\t\t\t// Owned objects are automatically garbage collected. For additional cleanup logic use finalizers.\t\t\t// Return and don't requeue\t\t\tlog.Info(\"SampleKind resource not found. Ignoring since object must be deleted\")\t\t\treturn ctrl.Result{}, nil\t\t}\t\t// Error reading the object\t\treturn ctrl.Result{}, err\t} else {\t\tlog.V(1).Info(\"Detected existing SampleKind\", \" sampleApp.Name\", sampleApp.Name)\t}\t// Check if the Deployment already exists, if not create a new one\tdeployment := &amp;appsv1.Deployment{}\tdeploymentName := sampleApp.Name\terr = r.Get(ctx, types.NamespacedName{Name: deploymentName, Namespace: sampleApp.Namespace}, deployment)\tif err != nil &amp;&amp; errors.IsNotFound(err) {\t\t// Define a new configmap\t\tdeployment := r.newSampleAppDeployment(deploymentName, sampleApp)\t\tlog.Info(\"Creating a new SampleApp\", \"SampleKind.Namespace\", sampleApp.Namespace, \"SampleKind.Name\", sampleApp.Name)\t\terr = r.Create(ctx, deployment)\t\tif err != nil {\t\t\treturn ctrl.Result{}, err\t\t}\t\treturn ctrl.Result{Requeue: true}, nil\t} else if err != nil {\t\treturn ctrl.Result{}, err\t}\tservice := &amp;corev1.Service{}\tserviceName := getServiceName(deploymentName)\terr = r.Get(ctx, types.NamespacedName{Name: serviceName, Namespace: sampleApp.Namespace}, service)\tif err != nil &amp;&amp; errors.IsNotFound(err) {\t\t// New service\t\tservice = r.newSampleAppService(deploymentName, sampleApp)\t\tlog.Info(\"Creating a new Service for SampleApp \", \"Service.Namespace\", service.Namespace, \"Service.Name\", service.Name)\t\terr = r.Create(ctx, service)\t\tif err != nil {\t\t\t//log failed to create\t\t\treturn ctrl.Result{}, err\t\t}\t\treturn ctrl.Result{Requeue: true}, nil\t} else if err != nil {\t\t//log failed to create\t\treturn ctrl.Result{}, err\t} else {\t\tlog.V(1).Info(\"Detected existing Service\", \" Service.Name\", service.Name)\t}One other important thing, we need to make sure the MockOperator has access to create, update, delete and read these two k8s resources.so we add these three lines and with the help of kubebuidler, the next time we run make generate manifests the right permissions will be give to the operator.//+kubebuilder:rbac:groups=\"apps\",resources=deployments,verbs=get;list;watch;create;update;patch;delete//+kubebuilder:rbac:groups=\"\",resources=pods,verbs=get;list;watch;create;update;delete;patch//+kubebuilder:rbac:groups=\"\",resources=services,verbs=get;list;watch;create;update;patch;deletev. Sample Image for the OperatorNow this part is optional, but needed so we can test the kubernetes deployment the operator manages. It is a simple docker image packaging, called sample-mock-service,for a custom HTML page in nginx. This can be found in the folder.vi. Testing with Ginko and GomegaTesting the Operator is specifically a large topic and it is not something I can fully expand on in this subsection.There are better resources like this and this.Our sample test will then look like this. It just uses BDD to confirm that when we create a SampleKind, a deployment also gets created.var _ = Describe(\"Deployment test\", func() {\tconst (\t\tname = \"deployment-test\"\t\tnamespace = \"default\"\t)\tContext(\"When SampleKind is created, Deployment is created\", func() {\t\tIt(\"allows deployment to be created and deleted\", func() {\t\t\tBy(\"set up deployment\", func() {\t\t\t\tskMockOperator := &amp;samplekind.SampleKind{\t\t\t\t\tObjectMeta: metav1.ObjectMeta{\t\t\t\t\t\tName: name,\t\t\t\t\t\tNamespace: namespace,\t\t\t\t\t},\t\t\t\t\tSpec: samplekind.SampleKindSpec{\t\t\t\t\t\tNodes: 1,\t\t\t\t\t},\t\t\t\t}\t\t\t\tExpect(k8sClient.Create(ctx, skMockOperator)).Should(Succeed())\t\t\t\tEventuallyWithOffset(10, func() bool {\t\t\t\t\tsmDeployment := &amp;v1.Deployment{}\t\t\t\t\terr := k8sClient.Get(ctx, types.NamespacedName{Name: skMockOperator.Name, Namespace: skMockOperator.Namespace}, smDeployment)\t\t\t\t\treturn err == nil\t\t\t\t}).WithTimeout(20 * time.Second).Should(BeTrue())\t\t\t\t//delete samplekind delete deployment\t\t\t\tExpect(k8sClient.Delete(ctx, skMockOperator)).To(Succeed())\t\t\t})\t\t})\t})})As you can see it is not extensive but since the MockOperator does little, the test coverage is pretty high as well.vii. Automated TestingsNow we need to add a simple integration test to confirm our operator works as expected. Using https://github.com/helm/kind-action we can set upa simple Kind cluster in K8s and then test the deployment of MockOperator and then using curl we can confirm if we can access the custom html page deployed in nginx.A section of the Github Action pipeline looks like this:name: Build and Test for Operatoron: workflow_calljobs: test: name: Test runs-on: ubuntu-latest steps: - name: Check out code uses: actions/checkout@v1 - name: Create k8s Kind Cluster uses: helm/kind-action@v1.3.0 - name: Login to Github Packages uses: docker/login-action@v2 with: registry: ghcr.io username: $ password: $ - name: Operator deployment run: | kubectl cluster-info kubectl get pods -n kube-system echo \"current-context:\" $(kubectl config current-context) echo \"environment-kubeconfig:\" ${KUBECONFIG} kubectl create ns mock-operator-system --save-config kubectl create secret generic regcred --from-file=.dockerconfigjson=${HOME}/.docker/config.json --type=kubernetes.io/dockerconfigjson -n mock-operator-system make deploy | grep created kubectl rollout status deployment mock-operator-controller-manager -n mock-operator-system --timeout=30s kubectl get crd | grep samplekind - name: Create deployment run: | kubectl create secret generic regcred --from-file=.dockerconfigjson=${HOME}/.docker/config.json --type=kubernetes.io/dockerconfigjson -n default kubectl apply -f ci/sample.yaml | grep \"lewis-sample\" sleep 5 ; kubectl get all kubectl wait pods --selector app.kubernetes.io/instance=lewis-sample --for condition=Ready --timeout=40s | grep \"condition met\" kubectl get po --show-labels | grep lewis-sample | grep \"1/1\" kubectl port-forward svc/lewis-sample-service 8080:80 &amp; sleep 5 curl localhost:8080 | grep mock - name: Delete operator deployment run: | kubectl delete samplekind lewis-sample | grep deletedConclusionHopefully you found this useful and are able to kick-start your operator journey with this. The source code for this MockOperator can be found here.Referenceshttps://kubernetes.io/docs/concepts/extend-kubernetes/operator/https://kubebyexample.com/learning-pathshttps://www.infracloud.io/blogs/testing-kubernetes-operator-envtest/https://betterprogramming.pub/write-tests-for-your-kubernetes-operator-d3d6a9530840" }, { "title": "Observability Stack with Grafana Loki Tempo and Prometheus", "url": "/posts/Obersevability-Stack-with-Grafana-Loki-And-Tempo/", "categories": "devops, sre, distributed, observability, monitoring", "tags": "kubernetes, grafana, loki, tempo, prometheus, monitoring, observability, slo, sla, sli, metrics, traces, logs", "date": "2022-04-20 00:00:00 +0200", "snippet": "ObservabilityObservability is tooling or a technical solution that allows teams to actively debug their system. Observability isbased on exploring properties and patterns not defined inadvance. Thisdefinitaion somewhat summarizes what Observability means.For your software system to be observable, the telemetry collected and how its process helps diagnose and investigatebetter.Observability is a close cousin of monitoring, but it’s actually different. Monitoring can be considered as a subset ofobservability. Whereas monitoring focuses on predefined metrics, logs and traces, observability focuses on thepredefined and the undefined. Of course, because one can not tell what will need to be debugged tomorrow. I am guessing,otherwise it will probably be fixed today.There are three main telemetry types considered to build observability.i) Metric: Measurements of a attributes of a resource or system.ii) Logs: Automatically generated timestamped records of various types of events. They could either be structured asjson or unstructured.iii) Traces: Traces are designed to track the way lifecycle or of an event or request with application orinfrastructure components. I wrote about it hereThere are so many tools to measure and record logs, traces or metrics. However, the best observability system should nottreat these telemetry types as independent. Since most of them are related in one way or the other. For example entriesin a log file could be related to a traceId in the distributed system which could affect one or two metric attributes ofa resource.In this post, I will show how we can build an observability stack that does not treat the telemetry types as independentsystems.Building the StackAs already mentioned, with each telemetry type we will us a different tool.i) Metric: Prometheusii) Logs: Lokiiii) Traces: TempoAnd then using Grafana bring all of them together. The source code canbefound here. I have also packaged everything using FluxCD, which canalso run on minikube so you should be able to test. To get started, first clone the repo and cd into it. Run make install-flux, which should install FluxCD. Once FluxCD is installed we can proceed and install the stack by running make install-allThis will first create a namespace and then install:i. prometheus-grafana ii. tempo iii. lokiAn important part to notice is the helm values for the prometheus-grafana installation.grafana: enabled: true adminPassword: password additionalDataSources: - name: Loki type: loki uid: loki url: http://loki.observability-system.svc.cluster.local:3100 access: proxy editable: true jsonData: derivedFields: - datasourceUid: tempo matcherRegex: ((\\d+|[a-z]+){5}(\\d+|[a-z]+){5}(\\d+|[a-z]+){4}) name: TraceID url: \"$${__value.raw}\" version: 1 - name: Tempo type: tempo uid: tempo url: http://tempo.observability-system.svc.cluster.local:3100 access: proxy editable: true version: 1The default value for the kube-prometheus-stack is to have prometheus enabled and set as the default datasource, so wedon’t need to patch that value.Secondly, we enabled Grafana using grafana.enabled: true and se the default password as p@sswordThirdly, we configured Loki, which is responsible for logs, as an additional datasource for Grafana and also specifyingthe url as http://loki.observability-system.svc.cluster.local:3100 because thats where our loki service runs from. Wealso have to extract TraceID from the logs using the regex ((\\d+|[a-z]+){5}(\\d+|[a-z]+){5}(\\d+|[a-z]+){4})The datasourceUid: tempo configuration allows us to link it to Tempo, that is if we can extract a TraceID base on theregex. Loki also has promtail automatically enabled because its default value is set to true in the helmchart, loki-stack. Promtail sets up a deamonset to allow loki access the logs.Now, that we have the visual aspect(Grafana), metrics (Prometheus), logs (Loki) covered, we need to configure tempo forout traces. Which is basically the service http://tempo.observability-system.svc.cluster.local:3100.Putting everything together, our set up should be running succesfully on minikube like below:Now that everything is running, we need to set up a sample app to use the Observability stack. That is wherethe hotrod app by jaegar comes in. Run make deploy-sample-app to set up hotrod. It deployment manifest looks at this, and instead of using Jaeger forDistributed Tracing, it rather uses Tempo. tempo.observability-systemapiVersion: apps/v1kind: Deploymentmetadata: labels: app: hotrod name: hotrodspec: replicas: 1 selector: matchLabels: app: hotrod template: metadata: labels: app: hotrod spec: containers: - args: - all env: - name: JAEGER_AGENT_HOST value: tempo.observability-system - name: JAEGER_AGENT_PORT value: '6831' - name: JAEGER_SAMPLER_TYPE value: const - name: JAEGER_SAMPLER_PARAM value: '1' image: jaegertracing/example-hotrod:latest name: hotrod ports: - containerPort: 8080 restartPolicy: AlwaysThis works out of the box. We can confirm this by generating random events on the hotrod service and see how theobservability stack reacts to it.We can also see the link between logs in Loki and using the traceID see how it relates to traces in Tempo.And we can also switch to prometheus to see metrics for our hotrod sample app.ConclusionTo summarize everything,i. Observability is important. ii. Its importance to not treat the telemetry types as silos as they may be related in one way or the other.iii. How to build a simple observability stack.iv. How to use Tempo for Distributed Tracing.v. Using Loki and promtail for logs." }, { "title": "Apache Kafka on Kubernetes with Strimzi Operator", "url": "/posts/Apache-Kafka-On-Kubernetes-Strimzi-Operator/", "categories": "distributed, devops, sre", "tags": "k8s, apache-kafka, kafka, kubernetes, fluxcd, gitops", "date": "2022-03-05 00:00:00 +0100", "snippet": "Strimzi is a kubernetes operator that enables a way to run an Apache Kafka cluster on Kubernetesin various deployment configurations with simple configurations. Meaning, we easily manage the lifecycle of our kafkadeployment.The details on how this is done can be found herebut to summarize: There are 4 main operators:1. Cluster Operator: Deploys and manages Apache Kafka clusters, Kafka Connect, Kafka MirrorMaker, Kafka Bridge,Kafka Exporter, Cruise Control, and the Entity Operator2. Entity Operator: Comprises the Topic Operator and User Operator3. Topic Operator: Manages Kafka topics4. User Operator: Manages Kafka usersThere are different configuration parameters that can be used for Strimzi asdetailed hereHowever, this example will focus on setting up a minimal cluster with minimal configurations and then confirm if we canconnect a client to Kafka.Kafka configuration with Kubernetes operator patternThe source can be found here.I am using FLuxCD to package everything, not just for GitOps, obviously this repo is not set correctly for an end to endGitOps with FluxCD. There’s a Makefile in the repo, which helps organize and how the PoC will be setup.First we need to install FluxCD for the crds using make---apiVersion: kafka.strimzi.io/v1beta2kind: Kafkametadata: name: strimzi-kafka-poc-minimal-ephemeralspec: kafka: version: 3.1.0 replicas: 1 listeners: - name: internal port: 9092 type: internal tls: false configuration: bootstrap: annotations: # external-dns.alpha.kubernetes.io/hostname: dummy domain for lb if external # external-dns.alpha.kubernetes.io/ttl: \"60\" config: offsets.topic.replication.factor: 1 transaction.state.log.replication.factor: 1 transaction.state.log.min.isr: 1 default.replication.factor: 1 min.insync.replicas: 1 inter.broker.protocol.version: \"3.1\" storage: type: ephemeral zookeeper: replicas: 3 storage: type: ephemeral entityOperator: topicOperator: {} userOperator: {} Confirm everything has been deployed correctly by running kubectl get kafka -n strimzi-kafka-poc.NAME DESIRED KAFKA REPLICAS DESIRED ZK REPLICAS READY WARNINGSstrimzi-kafka-poc-minimal-ephemeral 1 3 TrueThis is a confirmation that our Kafka and Zookeeper configuration has been successfully set upwith Strimzi.Note that, there a various configuration options to use the Strimzi Operator to set up Kafka ClustersThat can be found hereTesting Kafka Stream with Console from RedpandaNow that the Kafka cluster setup is completed, we would like to write one or more messages in a topicand confirm if we can consume the messages.Luckily there is a docker image from Strimzi to help us with this.Let us create a topic kafka-topic by running this command:kubectl -n strimzi-kafka-poc run kafka-producer -ti --image=quay.io/strimzi/kafka:0.23.0-kafka-2.8.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list strimzi-kafka-poc-minimal-ephemeral-kafka-bootstrap.strimzi-kafka-poc.svc.cluster.local:9092 --topic kafka-topicThen in the second terminal we launch a consumer and confirm if we can read messages from the topic kafka-topickubectl -n strimzi-kafka-poc run kafka-consumer -ti --image=quay.io/strimzi/kafka:0.23.0-kafka-2.8.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server strimzi-kafka-poc-minimal-ephemeral-kafka-bootstrap.strimzi-kafka-poc.svc.cluster.local:9092 --topic kafka-topic --from-beginningYou should see the messages produced consumed successfully. There is also a minimal setup for two clients that can connect to the Kafka.The first is Kafdrop and the second is Redpanda console" }, { "title": "Taking DataStax Enterprise for a Spin on Kubernetes", "url": "/posts/Cassandra-DSE-On-Kubernetes-With-Cass-Operator/", "categories": "distributed, devops, sre", "tags": "k8s, dse, java, spring-boot, cassandra, kubernetes, stateful", "date": "2022-02-09 00:00:00 +0100", "snippet": "1. IntroductionDataStax Enterprise (DSE) version 6.8 a hybrid cloud, that is can run on-premises or across regions on the cloud to giveall the capabilities of Apache Cassandra with enterprise tooling and expert support. In this particular concept it willbe deployed on AWS. Specifically EKS.2. Setting Up DSE Cluster on EKSTo set up a simple DSE cluster. DSE version : DSE 6.8.4 EKS : 1.21a. Download file https://github.com/malike/dse-on-k8s-with-java.git.b. cd into the deployment directory of the repository.c. Run make install-cass-operator to deploy the Cass Operator and also create the namespace dev-dse-poc. The CassOperator includes resources for the following: ServiceAccount, Role, and RoleBinding to manage permissions necessary to run the operator. CustomResourceDefinition (CRD) for the CassandraDatacenter resources used to set up the clusters managed by CassOperator. Deployment parameters to ensure the operator runs well. kubectl -n dev-dse-poc get po to confirm the operator is running. NAME READY STATUS RESTARTS AGE cass-operator-848fb7cd47-r5r6m 1/1 Running 0 2he. Create EBS StorageClass resource with the command make create-ebs-storage. This will create a StorageClass withthe definition.apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: server-storageprovisioner: kubernetes.io/aws-ebsparameters: fsType: ext4 type: gp2reclaimPolicy: DeletevolumeBindingMode: WaitForFirstConsumerIf you’re testing on minikube the StorageClass will not work for you. You should use something like this belowapiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: server-storageprovisioner: k8s.io/minikube-hostpathreclaimPolicy: DeletevolumeBindingMode: ImmediateThe values can be customized based on need.Verify the StorageClass is running perfectly.kubectl -n dev-dse-poc get storageclassNAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGEgp2 (default) kubernetes.io/aws-ebs Delete WaitForFirstConsumer false 5dserver-storage kubernetes.io/aws-ebs Delete WaitForFirstConsumer false 2hf. Create cluster and datacenters parameters with make create-data-center.apiVersion: cassandra.datastax.com/v1beta1kind: CassandraDatacentermetadata: name: dc1spec: clusterName: cluster2 serverType: dse serverVersion: \"6.8.4\" managementApiAuth: insecure: { } size: 3 storageConfig: cassandraDataVolumeClaimSpec: storageClassName: server-storage accessModes: - ReadWriteOnce resources: requests: storage: 5Gi config: jvm-server-options: initial_heap_size: \"1G\" max_heap_size: \"1G\" max_direct_memory: \"1G\" additional-jvm-opts: - \"-Ddse.system_distributed_replication_dc_names=dc1\" - \"-Ddse.system_distributed_replication_per_dc=3\"This is a single datacenter with 1 rack, and 3 nodes.Verify the datacenter is running by running kubectl -n dev-dse-poc get poNAME READY STATUS RESTARTS AGEcass-operator-848fb7cd47-r5r6m 1/1 Running 0 2hcluster2-dc1-default-sts-0 2/2 Running 0 2hcluster2-dc1-default-sts-1 2/2 Running 0 2hcluster2-dc1-default-sts-2 2/2 Running 0 2hAn output similar to this with all 4 pods running confirms a successful installation of DSE. One pod is for the CassOperator, which helpsmanage the lifecycle of the DSE cluster, the other 3 pods.Note that for setting up the DataCenter, racks must have identifiers. The number of racks created can not easily bechanged. The number of racks should match the replication factor in the keyspaces you plan to create.Define the storage parameters Define the storage with a combination of the previously provisioned storage class and sizeparameters. These values inform the storage provisioner how much room to require from the backend.3. Accessing The DSE Cluster with our Sample Java ApplicationTo access the DSE data center we’ll need to get the credentials. By default, Cass Operator creates a Cassandrasuperuser. A Kubernetes secret is created, named -superuser, which contains username and password keys.This case our cluster name is `cluster2`.kubectl get secret cluster2-superuser -n dev-dse-poc -o yamlapiVersion: v1data: password: base64-encoded-password username: base64-encoded-usernamekind: Secretmetadata: annotations: cassandra.datastax.com/watched-by: '[\"dev-dse-poc/dc1\"]' creationTimestamp: \"2021-10-12T10:35:28Z\" labels: cassandra.datastax.com/watched: \"true\" name: cluster2-superuser namespace: dev-dse-poc resourceVersion: \"6901797\" selfLink: /api/v1/namespaces/dev-dse-poc/secrets/cluster2-superuser uid: 0befbdd3-df81-4527-a2af-05af64fe0b06type: OpaqueThe source code is available here https://github.com/malike/dse-on-k8s-with-java.gitThen we deploy by executing the goal make deploy-app.4. Other Tools to manage the Cluster.There are additional opensource tools to take advantage of to manage our cluster aside from the CassOperator. For example: i. management-api-for-apache-cassandra, which provides a sidecar service layer that provides a set of operational actions on Cassandra nodes that can be administered. This is in use by the CassOperator as well. ii. reaper-operator which helps to schedule and orchestrate repairs of Apache Cassandra clusters iii. medusa-operator which helps manage backup/restore capabilities for Apache Cassandra" }, { "title": "Kubernetes Secrets Store CSI Driver POC", "url": "/posts/Kubernetes-Secrets-Store-CSI-Driver-PoC/", "categories": "devops, sre", "tags": "kubernetes, vault, secrets-store-csi", "date": "2022-01-18 00:00:00 +0100", "snippet": "Kubernetes Secrets Store CSI Driver POCContainer Storage Interface is a standard for exposing arbitaryy block andfile storage systems to containerized workloads in Kubernetes. There various third-party usesof the Container Storage Interface.The Secrets Store CSI driver is just one of the many drivers that takes advantage of the Container Storage Interface.It uses a list of compartible providers to make secrets managed outside Kubernetes available in Kubernetes via Container Storage Interface.There are a couple of providers supported which can work with Secrets Store CSI driver. AWS Provider : For AWS Secrets Manager Azure Provider : For Azure Key Vault GCP Provider : For Google Secret Manager Vault Provider : For Hashicorp Vault.This article and POC will focus much more on the Vault Provider.That is the goal of this article is so way to use and manageexternalized secrets stored on vault in hte Kubernetes cluster via the Kubernetes-native API.What are some advantages of doing this: No need to manage clients for where secrets are stored. Secrets Store CSI driver using the native Kubernetes-API means there’ll a more organized way of upgrading during cluster upgrades compared to multiple clients. The abstraction provided by Secrets Store CSI driver makes it easier to build a much more cloud agnostic infrastructure not tied to one Secret manager. The Twelve Factor App guideline for storing secrets as environment variables can be achieved with this.1. ArchitectureWe have a simple Go application, that will access secrets stored on vault provisioned by the Secrets Store CSI driver.2. Set up1. Configure Secrets Store CSI driver and Vault Provider To set up the POC clone the repository. cd into the directory. Make sure you’re connected to the right cluster. Install Secrets Store CSI driver into our cluster by running make install-csi. This should install the Secrets Store CSI driver into the cluster and also create the namespace dev-csi-poc. Install the Vault Provider using make setup-vault-provider.The deployment manifest for the Vault Provider looks like this. apiVersion: v1kind: ServiceAccountmetadata: name: vault-csi-provider namespace: dev-csi-poc---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: vault-csi-provider-clusterrolerules:- apiGroups: - \"\" resources: - serviceaccounts/token verbs: - create---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: vault-csi-provider-clusterrolebindingroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: vault-csi-provider-clusterrolesubjects:- kind: ServiceAccount name: vault-csi-provider namespace: dev-csi-poc---apiVersion: apps/v1kind: DaemonSetmetadata: labels: app: vault-csi-provider name: vault-csi-provider namespace: dev-csi-pocspec: updateStrategy: type: RollingUpdate selector: matchLabels: app: vault-csi-provider template: metadata: labels: app: vault-csi-provider spec: serviceAccountName: vault-csi-provider tolerations: containers: - name: provider-vault-installer image: hashicorp/vault-csi-provider:0.3.0 imagePullPolicy: Always args: - --endpoint=/provider/vault.sock - --debug=false resources: requests: cpu: 50m memory: 100Mi limits: cpu: 50m memory: 100Mi volumeMounts: - name: providervol mountPath: \"/provider\" - name: mountpoint-dir mountPath: /var/lib/kubelet/pods mountPropagation: HostToContainer livenessProbe: httpGet: path: \"/health/ready\" port: 8080 scheme: \"HTTP\" failureThreshold: 2 initialDelaySeconds: 5 periodSeconds: 5 successThreshold: 1 timeoutSeconds: 3 readinessProbe: httpGet: path: \"/health/ready\" port: 8080 scheme: \"HTTP\" failureThreshold: 2 initialDelaySeconds: 5 periodSeconds: 5 successThreshold: 1 timeoutSeconds: 3 volumes: - name: providervol hostPath: path: \"/etc/kubernetes/secrets-store-csi-providers\" - name: mountpoint-dir hostPath: path: /var/lib/kubelet/pods nodeSelector: beta.kubernetes.io/os: linux Ensure our Vault installation has Kubernetes auth enabled. Create a path for our secret, vault secrets enable -path=service-a kv-v2 Put in a sample password vault kv put service-a/database password=secret1234 for our made up service. Create a vault policy to enable us read the secrets for our made up service. vault policy write service-a-policy -&lt;&lt;EOFpath \"service-a/data/database\" {capabilities = [\"read\"]}EOF Create a Kubernetes Auth role that grants that allows for the reading of secrets to the service account in a specific namespace. vault write auth/kubernetes/role/csi \\bound_service_account_names=secrets-store-csi-driver,goservice-csi-serviceaccount,vault-csi-provider \\bound_service_account_namespaces=dev-csi-poc \\policies=service-a-policy ttl=720m Now we’ve set completed the setup Secrets Store CSI driver, Vault Provider and enabledkubernetes auth role to the ServiceAccount in the namesapce ev-csi-poc2. Deploy ServiceThe setup is ready for the deployment of the service.The deployment manifest for our simple Go service has a custom resourcecalled SecretProviderClass, which was installed when we deployed Secrets Store CSI driver.It looks something like this.apiVersion: secrets-store.csi.x-k8s.io/v1alpha1kind: SecretProviderClassmetadata: name: goservice-vault-providerspec: provider: vault parameters: roleName: \"csi\" vaultAddress: \"http://vault.vault:8200\" vaultSkipTLSVerify: \"true\" objects: | - secretPath: \"service-a/data/database\" objectName: \"password\" secretKey: \"password\"Few things to note are: vaultAddress : The URL to our vault. roleName : The kubernetes role created in vault to access the secrets. objects: Details of the secrets we want to mount.We can also mount multiple secrets, certs in the same cluster.Now to use it as environment variables, we mount SecretProviderClass as a volume. env: volumeMounts: - name: goservice-store-inline mountPath: \"/mnt/secrets-store\" readOnly: true volumes: - name: goservice-store-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: goservice-vault-providerTo deploy this, let’s run make kube-deploy to install the service into the cluster.3. Test SetupTo test we need to confirm if the secret path configured in vault is accessible in the pod. Run kubectl get po -n dev-csi-poc to get pods in the namespace. You should have something like this:NAME READY STATUS RESTARTS AGEcsi-secrets-store-secrets-store-csi-driver-6zkk7 3/3 Running 0 2m49sgoservice-csi-c8b66664d-kdgjk 1/1 Running 0 88svault-csi-provider-hww87 1/1 Running 0 2m1s Connect to the pod, with kubectl exec goservice-csi-c8b66664d-kdgjk -n dev-csi-poc -- ls /mnt/secrets-store. Since we’ve mounted our vault secrets to this path /mnt/secrets-store You can proceed and cat the password to confirm if the secret stored in vault will be accessible.kubectl exec goservice-csi-c8b66664d-kdgjk -n dev-csi-poc -- cat /mnt/secrets-store/password The Go service also provides an httpendpoint to read contents of the file /mnt/secrets-store/password. This can be accessed by getting the NodePort and then accessing the path /vault. You should see the contents of password file displayed.kubectl get svc -n dev-csi-poc with the output :NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEgoservice-csi NodePort 10.104.91.25 &lt;none&gt; 8080:31510/TCP 12mcurl http://localhost:31510/vault as an alternative checkpoint.Few things to note, the CSI driver is invoked by kubelet only during the pod volume mount.So subsequent changes in the SecretProviderClass after the pod has started doesn’t trigger an update to the content in volume mount or Kubernetes secret. There’s also a way to enable autorotation of secretswithout having to restart the pod. However, this is still in alpha." }, { "title": "Introduction to Software Development on Kubernetes - Part 1", "url": "/posts/Starting-With-Kubernetes-Docker-For-Mac-1/", "categories": "devops, sre", "tags": "kubernetes", "date": "2020-06-26 00:00:00 +0200", "snippet": "1. Setting up KubernetesKubernetes is fast becoming, if not already, the OS for building cloud native applications. As someone put it, “the platform for building platforms”. As such people will want to replicate their production during dev. If possible at a lower cost. Imagine every member of the team having a “cluster” to play with.There are many alternatives like KinD, which I’ll write about in another post, but this post is focused on Docker Edge for Mac. As the name suggests it’s purposely for Macs. In this post we get to install and take it for a spin. We’ll get to deploy a sample application and get to use some simple commands to manage our clusters.The installation is pretty simple; if you follow the steps here. Now we that we have it installed we’ll need kubectl.kubectl is the kubernetes cli. It helps us run commands on clusters to manage, monitor and deploy application or resource. You can find some of the cool stuff you can do with it here. If you prefer a web based interface instead for managing your cluster there’s an option as well. The Kubernetes dashboard is purposely for that. We can easily install it with kubectl create -f command. This command expects you pass a yaml file that describes the resources you want to create and deploy. Fortunately there’s a yaml file for that here.kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta1/aio/deploy/recommended.yamlNow let’s confirm if our dashboard is installed properly and running with this. kubectl get pods --all-namespaces | grep dashboard.This command lists all pods kubectl get pods --all-namespaces by greping the results we can find all pods which are dashboards. What are pods?, what about the --all-namespaces and finally how do we access the dashboard?. I’ll get to that shortly.We know the dashboard is running but how do we access it?, by runningkubectl proxyThe dashboard should be accessible at http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/login.After installation of the dashboard, you’ll have minimal set of privileges. There’s a lot more you can do but since this is an introduction let’s just limit it to how you can log in and access the dashboard.First option is to use kubectl to get all secrets of available service-accounts with kubectl -n kube-system get secret.We’ll probably get something similar. Now different service accounts have different permissions so we probably want one with full or enough permissions to navigate the dashboard effectively.We can get a token with kubectl -n kube-system describe secrets replicaset-controller-token-xxxxx, make sure you get the full name correct for replicaset-controller-token. You can select the token option, paste the token in the text box and log in. For local development this option can get annoying sometimes due to the way the token expires.Another option is skipping authentication altogether, again stressing that this is only an option because we are setting up to Kubernetes to develop locally. This can be done by just adding ths flag when starting the dashboard --enable-skip-login. Simple as that.After successful login, you should see something like this.After following this steps, we can celebrate. Because we’ve successfully run Kubernetes, installed the Kube dashboard on our Mac. Now we can move to the next stage of developing a simple application to run on our local Kubernetes cluster.2. Developing on KubernetesNow we’ve set up Kubernetes locally, how do we deploy our source. We can start of by running a simple nginx server. But before we deploy, let’s look at some objects in Kubernetes and what they mean.Pods : A Pod is the Kubernetes object that represents a group of one or more containers. The smallest deployable object in the Kubernetes object model.ReplicaSets : A ReplicaSet is responsible for a group of identical Pods, or replicas. If there are too few (or too many) Pods, compared to the specification, the ReplicaSet controller will start (or stop) some Pods to rectify the situation.Service : A way to expose a set of pods as a network service to be easily accessible.Deployments : Deployment objects records information about the container image and whatever else it needs to know to start and run the container.There are more but this is just about what we need to deploy our first application.Let’s create a deployment called demo-nginx with this:kubectl create deployment demo-nginx --image=nginxThe --image options specifies the image we want to deploy in the pod. Once this completed let’s verify our deployments and pod with kubectl. Usingkubectl describe deployments demo-nginxkubectl describe pod demo-nginxDo note that we can also do this from the dashboard by just navigating pretty straightforward and simple Web UI.Now how de we access nginx, we’ll need to expose our deployment with a service resource using the LoadBalancer type to help us access it. We know that the default port for nginx is 80, so we just need to port-forward the to port 80.kubectl expose deployment demo-nginx --type=LoadBalancer --name=demo-nginx --port=8080 --target-port=80.We can verify and confirm is nginx is accessible on port 8080. Also we can describe the services with kubectl or just visualize on the dashboard.Next up in the series we’ll build a custom application, package with helm charts with custom services and deploy in our cluster. We’ll also learn to scale our deployments, configure observability and monitor our clusters.REFERENCEShttps://kubernetes.io/docs/reference/" }, { "title": "Load Testing with k6.io", "url": "/posts/Load-Testing-With-K6/", "categories": "", "tags": "ci, cd, k6.io, load, testing", "date": "2019-07-05 00:00:00 +0200", "snippet": "Load testing is about putting production level demand on a system to simulate production. This helps measure and define requirements for best performance. I’ll show how we can easily set up one of the load testing tools out there to achieve this.K6 is a developer centric open source load and performance regression testing tool for testing the performance of your cloud native backend infrastructure: APIs, microservices, serverless, containers and websites. It’s built to integrate well into your development workflow and CI/CD automation pipelinesI like it because of the ease to integrate in a CI/CD pipeline. The installation guide is pretty straight to the point. Which can be automated with ansible or terraform when setting up QA environments.import { check, sleep } from \"k6\";import http from \"k6/http\";export default function() { let res = http.get(\"http://localhost/\"); check(res, { \"is OK \": (r) =&gt; r.status === 200 });};This is a simple script to load test nginx running on port 80. We can start it with this command :k6 run --vus 30 --duration 60s loadtest_nginx.jsWhich basically means 30 virtual users trying to access nginx on port for 60 seconds. When completed we’ll mostly likely see something like this.K6.io supports four different metric types : Counter, Gauge, Rate and Trend. You also have the ability to create custom metrics.1. Counter : Cumulative metricimport http from \"k6/http\";import { Counter } from \"k6/metrics\";let CounterErrors = new Counter(\"Errors\");export default function() { let res = http.get(\"https://test.loadimpact.com\"); let contentOK = res.html(\"h2\").text().includes(\"Welcome to the LoadImpact.com demo site!\"); CounterErrors.add(!contentOK);};2. Gauge : Keep track of the latest value onlyimport http from \"k6/http\";import { Gauge } from \"k6/metrics\";let GaugeNginxContentSize = new Gauge(\"nginx gauge\");export default function() { let res = http.get(\"http://localhost\"); GaugeNginxContentSize.add(res.body.length);};3. Rate : Tracks percentage of values in a series that are non-zeroimport http from \"k6/http\";import { Rate } from \"k6/metrics\";let RateNGINX = new Rate(\"nginx rate\");export default function() { let res = http.get(\"http://localhost\"); let httpStatusOK = res.html(\"h1\").text().includes(\"Welcome to nginx!\"); RateNGINX.add(httpStatusOK);};4. Trend : Calculating statistics on the added values (min, max, average and percentiles)import http from \"k6/http\";import { Trend } from \"k6/metrics\";let TrendNginx = new Trend(\"nginx\");export default function() { let res = http.get(\"http://localhost\"); TrendNginx.add(res.timings.duration);};If I forgot to mention this, you can also create custom metrics. But how do we automate this and include this in our CI/CD pipeline. This is particullary for cases whereby there’s a certain threshold of requests our application can handle and we don’t want any update setting us back.K6.io supports creating thresholds as well. For example a simple threshold to make sure our application has than 0.5 failure when bombarded with requests from 30 virtual users in 60 seconds. It’s of type rate. k6 run --vus 30 --duration 60s loadtest_nginx_threshhold.jsimport http from \"k6/http\";import { Rate } from \"k6/metrics\";var failedRate = new Rate(\"failures\");export let options = { thresholds: { \"failures\": [\"rate&lt;0.5\"], }};export default function() { let res = http.get(\"http://localhost/\"); failedRate.add(res.status != 200);};We can use the results as a gate keeper to either continue the process in the CI/CD pipeline or not. It’s pretty simple.REFERENCEShttps://k6.io/" }, { "title": "KubeCon Europe 2019 : My KubeCon Experience and What is Next", "url": "/posts/My-KubeCon-Experience/", "categories": "stuff", "tags": "conference, cncf-ghana", "date": "2019-05-29 00:00:00 +0200", "snippet": "KubeCon + CloudNativeCon Europe 2019 was not my first international conference invite but it was the first one I actually got to go. The other one I couldn’t get to go was Elasti{ON} 2018 because of VISA issues. Knowing KubeCon + CloudNativeCon Europe 2019 was happening in Europe I knew I had to go. Where else will you meet all cloud native enthusiasts, researchers, engineers I mean every one working to make building of cloud native applications easier. I applied for a Diversity Scholarship and fortunately for me I was amongst the 56 people selected to be part of KubeCon 2019 at Barcelona.This post is just to tell you about my experience, why you should go for the next one, what I plan to do next and also to say thank you to all the sponsors who made it possible.1. Let the games begin The first thing you’ll notice will be the Code of Conduct. It was literally everywhere, so everyone adheres to the rules of engagement.I registered for KubeSec as well and it was just amazing the sessions we had; from designing and implementing secure CI/CD pipelines using tools like manifesto and notary, learning from the challenges faced in implementing a multi-tenancy cluster and different ways to secure your Kubernetes cluster. One thing that I’m really looking out for is what Nick Smith mentioned in his presentation; “Bringing end user identity to the Service Mesh”. Coupled with the launch of Service Mesh Interface I’m excited about the interoperability of any Service Mesh with Kubernetes and how it will work with end user authentication to see what else we will be able to achieve from this aside the reliability, observability and security that we already get from service meshes.Later that evening, I went for an Istio Meetup at Carrer d’En Cortines. It was packed and didn’t fall short of inspiration. Stories of what people are doing with Istio and what’s next for Istio from the creators.There were so many sessions to pick from. Mostly centered around how people used cloud native opensource tools to improve certain deliverables, mistakes they made using the cloud native tools and how you shouldn’t do what they did or just introduction to new ways of doing things. There were sessions on Mental health, Lightning talks on diverse topics and also a chance to meet maintainers of some of the tools we use in the cloud native space. Imagine breaking the slack/zoom or gitter barrier to finally meet and talk with maintainers you’ve worked with. I got to meet Bogdan Drutu who is one of the maintainers of OpenCensus, I contributed an elasticsearch trace exporter to help visualize distributed traces in Kibana. By the way if you use OpenCensus kindly read about the merger between OpenCensus and OpenTracing to form OpenTelemetry.I couldn’t believe I met Joe Beda and got a signed book from him as well.Oh, there was time for fun as well. This year we got lucky to celebrate Kubernetes 5th birthday. 5 years for the “Cloud Native OS”, got to see the beautiful town of Poblo de Escober and then the AWS party on the last day. Made new connections, found new opensource technologies to contribute to and use for different projects. It was simply amazing.What next : CNCF Ghana ChapterThere’s this qoute by Kevin Spacey, not sure if it his character Frank Underwood on House of Cards or Kevin Spacey himself. But to paraphrase ,“If you’re lucky enough to do well,it’s your responsibility to send the elevator back down”.After all these amazing experiences, some of which I’ve not included in this post, it will be selfish not to help create an avenue to bridge CNCF opportunities and also a platform to share knowledge about opensource tools under the CNCF umbrella. This is not just for people who already know these but people who are also willing to start in the cloud native space.Chatting with Kasper Nissen during one of the “Meet the Ambassador” sessions, I enquired about starting a CNCF Ghana chapter. Learnt a lot about how he started #CloudNativeAarhus and how they’ve been able to keep an active community this far.I’m really excited for what myself and a couple of people; Adane Nana, Addico Samuel, Banini Mawumefa, Boadu Kwabena, Obugyei Eunice and Quartey Papafio Winnie are ready to do with our local CNCF Meetup in Ghana. We hope to share knowledge and also help people to contribute to some of the opensource technologies under the cncf.io umbrella. Just like KubeCon + CloudNativeCon we will do our utmost best to make our Code of Conduct very visible to build an all inclusive CNCF Ghana chapter.We are not certain of what the challenges ahead hold for us but it’s better to have start and fail than not to start at all. Give us a follow and you can help." }, { "title": "ChatOps : Building A Custom Slackbot", "url": "/posts/ChatOps-Slack-AWS-Integration/", "categories": "devops, sre", "tags": "aws, slack", "date": "2019-05-12 00:00:00 +0200", "snippet": "ChatOps“ChatOps is a new operational paradigm - work that is already happening in the background today is brought into a common chatroom. By doing this, you are unifying the communication about what work should get done with the actual history of the work being done. Things like deploying code from Chat, viewing graphs from a TSDB or logging tool, or creating new Jira tickets…all of these are examples of tasks that can be done via ChatOps”Although the use cases vary for different organizations some of the common things ChatOps helps with simple are automation tasks, actionable notifications and transparency in distributed teams. There are some advance use cases like using ChatOps for deployments to telling a joke in the company message board to lighten the mood.1. Creating A ChatOpsTo build one Chatbot for ChatOps for your organization there are certain checklists that need to be marked before you start and there are :a) A chat platformThere is no Chatbot without a Chat platform. The common one is Slack but there are other ones like Flowdock. There are open-source ones like Mattermost which can to be self-hosted. Whatever the choice it should be accessible across web, mobile, and desktops to respond in time to actionable alerts etc..b). Choose/Build a chatbotSecondly, we’ll need a chatbot. This is the foundation of your ChatOps so there a lot will go in for making your choice. It doesn’t need to be smart like Tony Stark’s assistant J.A.R.V.I.S it should just be able to accept commands and respond by initiating actions. This doesn’t need to be overly complicated so far as it can get the job done.If you don’t have time to build one from scratch there are tons of open-source chatbots with adaptors to work with different chat platforms. They offer different functionalities and mostly come with different extensions to upgrade or improve the capabilities of your chatbots.Some of these can be found here.c). Integrate and automate workflowsOnce the chatbot is ready, we’ll need to define daily workflows, the mundane and repetitive ones, that we need to be automated and integrate them. If you’re building your chatbot from scratch these are features you have to design and code yourself. But if you’re using an already developed one you can search for extensions or search ways to write custom scripts to work with your chatbot. Since most of the chatbots have APIs to integrate custom functionalities.An example is Hubot, a chatbot developed at Github, which has ways extending your bots functionalities by writing .cofee or .js scripts.One important aspect to consider when integrating all these extensions to automate workflows is authentication, authorization and audit. It’s very important to not expose every part of your system to every user via the integrations. Same way not every user can restart production services same way you need to incorporate authorization via the chatbot.d). Structure to get everyone on boardFinally design a structure to keep the chatbot conversations as organized as possible. Imagine a use case where by chatbot spams the marketing team about CPU usage of a service. This may be considered as spam thereby affecting company-wide adoption. To create a culture that buys into ChatOps proper grouping on conversations from the bot needs to be created and gradually features and functionalities are integrated into daily workflows per team/department needs.Onboarding should be made as simple as possible with actionable alerts. For example if there’s an alert that a service is down, the true value of chatbot should also provide a means to start the service from the chat platform. As much as possible onboarding should require company /organization sso flow so it doesn’t take another sign up form before people can start using it.2. Building A Sample ChatBot : AWS S3 and IAM Policy Monitoring with Slack Slash CommandsBy putting the cart before the horse, let’s tackle packing first it has to be simple to install no matter the environment, so we’ll use a container to help abstract this. You can find Dockerfile files. Using travis-ci we can automate the building and release of the bot to DockerHub.Now we just want a bot to be able to do simple things, like running a bash or shell script from a bastion host. I’ve decided to use Java Spring for this, you can use any language you’re comfortable with.I’ve added two utility scripts as well to help test, one that lists all “public” S3 buckets on AWS and the other lists all users without MFA. Both scripts are specific to AWS and use the aws-cliNow the main part, integrating the bot to work with Slack. There’s an HTTP endpoint /slackbot which we’ll have to configure for our slackbot to receive request or better known as Slash Commands. Slash Commands are basically shortcuts for specific actions in Slack. Type a slash command in the message field, select enter to send, and that’s it. You’ve performed a task in one simple step. In our case we’ll want a slash command as simple as/slackbot ping UAT to ping a UAT server we’ve already mapped to the term “UAT” with response that UAT server is healthy. We can also use it to execute bash scripts, which I’ve tested with two custom bash scripts one to get list of public S3 buckets on AWS and the other to list users with MFA turned off on AWS.Once we get the results of the bash scripts are generated we can push it to slack using slack-cli. This will then send a successful callback using the slack API to the initiator on slack.But to have actionable notifications we should be able to either disable the public policy of the S3 bucket or notify users to turn on MFA, or restart a service after a ping returns UNHEALTHY HOST.This is pretty much a simple slack bot(in progress) but hopefully it will give you ideas to build upon this or start a fresh one for your organizations. It has a simple workflow and will have actionable notifications so users can easily respond to alerts anywhere they are just by using Slack. Source code available on Github, if you’ll like to help finish it.Referenceshttps://docs.stackstorm.com/chatops/chatops.html" }, { "title": "Using Terraform on GCP, AWS and Physical Server", "url": "/posts/Using-Terraform-On-GCP-AWS-And-Physical-Server/", "categories": "devops, sre", "tags": "terraform", "date": "2019-05-11 00:00:00 +0200", "snippet": "Terrafrom“Terraform is an open-source infrastructure as code software tool created by HashiCorp. It enables users to define and provision a datacenter infrastructure using a high-level configuration language known as Hashicorp Configuration Language (HCL), or optionally JSON. Terraform supports a number of cloud infrastructure providers such as Amazon Web Services, IBM Cloud (formerly Bluemix), Google Cloud Platform, Linode, Microsoft Azure, Oracle Cloud Infrastructure, or VMware vSphere as well as OpenStack.”For me that’s the one thing that sets Terraform apart from the others. It supports other cloud infra and doesn’t take a lot of effort to switch config from one to the other. Some people argue that to be cloud agnostic is a myth and just a marketing term, which may be partly true, but there many advantages to not tightly coupling everything in your infrastructure to one cloud service provider. For example AWS CloudFormation is cool but restricts you to just AWS. Terraform however supports many providers and allows you to provision resources on different servers and if abstracted properly you can use same terraform files to different cloud providers with minimal changes.Terraform 101To help us appreciate this let’s create an nginx server on 3 types of servers, linux box, GCP and AWS.Before that a short Terraform 101 for more details you can read more here.1. Variables : Input variables are used to define values that we can use to configure your infrastructure. There are different formats:i. stringStrings mark a single value per structure and are commonly used to simplify and make complicated values more user-friendly. Below is an example of a string variable definition.variable \"username\" { default = \"malike_st\"}and by using the template below we can substitute values for our variables in the *.tf files.username = \"${var.username}\"ii. listList is a collection type that allows us to use multiple values for a variable and can easily be read by their indexes.variable \"servers\" { default = [\"server\", \"server1\", \"server2\"]}and using indexes we can pick the value we want.server = \"${var.servers[0]}\"iii. mapMaps represent another type of collection defined by key value pairs.variable \"region_zone\" { type = \"map\" default = { \"us-central1\" = \"us-central1-b\" \"us-west2\" = \"us-west2-a\" }}We can access any value by it matching key.deployment_zone = \"${var.region_zone[\"us-central1\"]}\"iv. booleanThe last type of variable types is boolean. Simple true or false values we can use in our infrastructure set upvariable \"set_password\" { default = false}Variables can be set in a separate file, tfvars and used during deployment. This variable file will have passwords and other sensitive stuff we wouldn’t want to share with the world. It can be kept outside the terraform files so it’s not mistakenly versioned. By passing our variable file terraform apply -var-file=../../aws_vars.tfvars we can set the variables to use for a deployment.2. Output Variables : Output variables helps us get useful information about your infrastructure. For example to return the public ip of an EC2 instance after deployment. Since most of the computation is done during deployment, Output variables help us peek into the deployment as it happens.output \"aws_instance_public_dns\" { value = \"${aws_instance.nginx.public_dns}\"}3. Providers : A provider on Terraform provides an abstraction through which Terraform can work with the underlying resources of the IaaS (e.g. AWS, GCP, Microsoft Azure, Physical servers, Vagraant), PaaS (e.g. Heroku, CloudFoundry).A list of providers supported by Terraform can be found here but that’s not an exhaustive list.An AWS provider provider \"aws\" { access_key = \"${var.aws_access_key}\" secret_key = \"${var.aws_secret_key}\" region = \"us-east-1\"}A GCP provider provider \"google\" { credentials = \"${file(\"${var.access_file}\")}\" project = \"${var.project_id}\" region = \"us-west2\"}Resources : A resource is basically any object under a provider (IaaS, PaaS) that we can can set up with a set of parameters. For example an EC2 instance is a type of resource on AWS and compute a resource on GCP.An AWS resource resource \"aws_instance\" \"nginx1\" { ami = \"ami-c58c1dd3\" instance_type = \"t2.micro\" subnet_id = \"${aws_subnet.subnet1.id}\" vpc_security_group_ids = [\"${aws_security_group.nginx-sg.id}\"] key_name = \"${var.key_name}\"}A GCP resource resource \"google_compute_instance\" \"nginx\" { name = \"nginx-vm-${random_id.instance_id.hex}\" machine_type = \"f1-micro\" zone = \"us-west2-a\"}Simple Terraform Script To NGINX serverPutting everything together here’s a simple terraform set up to install NGINX server on local server, AWS and GCP with security groups/ firewall restrictions. You can see the similarities between all three scripts and the main difference being in the provider and resources because obviously you cant use AWS resources on GCP and vice-versa but because of terraform’s HCL it’s easier (imo) to use than the JSON or YAML.For example this is the same resource declared in HCL (Terraform) and YML (CloudFormation)HCL : Terrafrom resource \"aws_instance\" \"nginx\" { ami = \"ami-c58c1dd3\" instance_type = \"t2.micro\" subnet_id = \"${aws_subnet.subnet1.id}\" }YML : CloudFormation Resources: NGINXEC2Instance: Type: \"AWS::EC2::Instance\" Properties: InstanceType: \"t2.micro\" ImageId: \"ami-c58c1dd3\" SubnetId: !RefNginxSubnet1But why would one want to set up their environments on both AWS GCP or locally?, I don’t have all the answers but it’s quite easy to see different concerns by different teams for example one company will love to set up their environments locally and and not strictly restrict it to the cloud.One other thing to like about Terraform is it keeps states in a file terraform.tfstate, where it keeps details about the terraform version and other meta data to improve performance of infrastructure and keep your infrastructure sane.Terraform validates your configuration with terraform plan, where it will give you a summary of everything it will be doing before it actually does it with terraform apply.Terraform is really cool and it’s opensource and I personally love it for being cloud agnostic and HCL, if you find it interesting as well and want to learn more https://learn.hashicorp.com/terraform/.REFERENCEShttps://www.terraform.io/docs/" }, { "title": "Continuous Integration, continuous Deployment with AWS Using EKS, CodeBuild, CodePipeline, ECR and CloudFormation", "url": "/posts/CI-CD-AWS-Infrastructure-For-Kubernetes-Cluster-Using-CloudFormation/", "categories": "devops, sre", "tags": "distributed, codepipeline, codedeploy, codecommit, codebuild, cloudformation, microservice, k8s, aws, eks", "date": "2019-02-01 00:00:00 +0100", "snippet": "Building up on our previous article, where a simple cloud native/ distributed system application was built to run on kubernetes using Travis CI, DockerHub locally. What if we move this infrastructure to the cloud, AWS for this instance. Using AWS tools to accomplish the same thing and deploy also test AWS EKS(AWS’s Kubernetes offering).This will include setting up a CI/CD pipeline and setting up the infrastructure to host the application on staging and production.This article will be treated in 2 parts:a. Part One : CI/CD Pipelineb. Part Two : Set up of infrastructure for deploymentBut we before we delve in, let me talk about AWS CloudFormation.CloudFormation“AWS CloudFormation provides a common language for you to describe and provision all the infrastructure resources in your cloud environment”This is termed as Infrastructure as Code, and can help us automate our CI/CD pipeline as well as provision of resources for each step of the process. A CloudFormation template can be created using the visual builder present in AWS using drag-and-drop, or coding in json or yml.In short, what AWS CloudFormation offers us is a way to combine AWS resources to build our CI/CD pipeline and also AWS resources to manage the infrastructure to host the application.a). CI/CD Pipeline1. AWS CodeCommit : This a fully-managed source service for git-based source repositories.2. AWS CodeBuild : This a fully-managed CI server. This includes compiling source code, running test and producing software packages.3. AWS CodePipeline: This a fully-managed continous delivery service for automation of build,test,and deployment.b). Set up of infrastructure for deployment4. AWS ECR : This is a fully-managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images5. VPC : This is to provide a virtual cloud for our cluster.6. EKS Cluster : Amazon Elastic Container Service for Kubernetes. I’ll talk more about this in a later section.7. EC2 Instances : EC2 nodes to run the Kubernetes podsPart One : CI/CD PipelineThe CI/CD pipeline will use AWS resources to pick the source code from Github using AWS CodeCommit then using AWS CodeBuild build a docker image and then using AWS CodePipeline push the docker image to AWS ECR.To create the CI/CD pipeline we can log into AWS and create it from the console UI create ci/cd pipeline on aws ui or just use a CloudFormation template to to join all the resources in our template.The CloudFormation template configured to use AWS CodePipeline to will look like.If you’re new to CloudFormation you can read more here or here.From the Pipeline template, you notice we have three sections, Parameters, Resources and Outputs.Parameters enable us to use custom values to your template each time you create or update a stack. For our template this would be details for AWS CodeCommit to pull the source code from Github.From Github, Using this url https://github.com/settings/tokens we can get the parameters AWS CodeCommit will use.i. Github token name ii. Github username iii. Github repository name iv. Github source branch name Resources declares AWS resources we’ll like to include in the stack. The resources that enable us to build th CI/CD pipeline. It is in 3 parts, the first one been CodePipeline, this describes the pipeline from Github and using the infrastructure CloudFormation template to setup staging and production environment, the second CodeBuild, this also describes the resources used to build the source and the last part of resources describes all the IAM policies needed.Outputs which is not mandatory, is used to fetch values that can be reused in other templates. pipeline aws cloudformation visual To configure AWS CodeBuild to build the source code we need to add a buildspec.yml to the root of the source. This is is just to help AWS CodeBuild on what it needs to do to build the source. There are 3 phases involved to build a source pre_build, build and post_build.For our source the goal we want to achieve for each build phase can be summarized as :pre_build : This is the commands we need to run before build. In our case it doesLog in to Amazon ECR and set the repository URI to your ECR image and add an image tag with the first seven characters of the Git commit ID of the source.build : This is commands to build the source. Build the Docker image and tag the image both as latest and with the Git commit IDpost_build : Push the image to your ECR repository with both tags.If you check the source repository I’ve added some scripts for to help our structure of the repository.This being the last part of configuring AWS resources to build a CI/CD pipeline we can move on test. How to test this?, lets make slight changes to the source on Github and verify if there will be new images on ECR. After committing the changes as we can see : new images on ECRThis confirms our CI/CD pipeline works perfectly. But we’re not done. We still need to pick the images from AWS ECR and deploy to AWS EKS.Part Two : Infrastructure Set Up and DeploymentWe left of the previous part with docker images uploaded to ECR after source code was pulled from Github and built. All with our CI/CD pipeline. Next step, set up an EKS infrastructure to pick the docker images, set up the infrastructure on EKS to deploy the application. We’ll need a couple of resources to get this done and once again AWS CloudFormation comes to the rescue.To deploy this on EKS we’ll need, a VPC, EKS and 3 EC2 instances. Since our source code is made up a java, go and html microservices.Putting everything together we have the infrastructure CloudFormation template.Putting the infrastructure together with the pipeline we can set up a staging environment and a production environment. But due to cost (:money_with_wings:) I’m only going to set up and staging environment.Summary : Continuous Integration/ Continuous DeploymentTo summarize what we’ve done so far :1. Use CloudFormation to set up infrastructure for CI/CD pipeline.2. Set up a Github repository, using AWS CodeCommit we can pick changes from the master branch Github.3. The Docker image is pushed to Amazon ECR after a successful build and/or test stage.4. CloudFormation sets up EKS clusters for staging.5. EKS updates the deployment pods using a rolling update strategy by picking the images from Amazon ECR.6. EKS updates the deployment pods using a rolling update strategy by picking the images from Amazon ECR automatically. (Technically this wasn’t done, due to cost )Hopefully you understand how to use strictly AWS resources to build CI/CD pipeline as well manage the infrastructure for such a deployment. Source code available on Github.Referenceshttps://aws.amazon.com/blogs/compute/continuous-deployment-to-amazon-ecs-using-aws-codepipeline-aws-codebuild-amazon-ecr-and-aws-cloudformation/" }, { "title": "Tracing Spring Boot Hazelcast and MongoDB", "url": "/posts/Tracing-Spring-Boot-Hazelcast-MongoDB/", "categories": "tech", "tags": "tracing, sleuth", "date": "2018-12-20 00:00:00 +0100", "snippet": "1. HazelcastHazelcast is an in-memory data grid (IMDG). In-memory data grids are distributed stores which rely primarily on the RAM for storage across a cluster for better performance. This makes IMDG’svery good caches, managing sessions across microservices, real time monitoring and so on.An IMDG like Hazelcast, augments the core DB no matter what software it is. This reduces round trips to the the database to have better performance. This can also work as layer before APIs as well.Hazelcast is also distributed, this makes it a right fit for microservice architectures.2. Tracing and Hazelcast and MongoDBIn previous post I explained what distributed tracing is and how to do it with Opencensus, ELK and Spring Sleuth.To make a case why we need an IMDG like Hazelcast, I thought it be easier to to trace simple write and read requests to both Hazelcast and MongoDB.Setting up Zipkin to run on port 9411.Now to set up a simple Spring Boot project to use Hazelcast we need to add these dependencies&lt;dependency&gt; &lt;groupId&gt;com.hazelcast&lt;/groupId&gt; &lt;artifactId&gt;hazelcast&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.hazelcast&lt;/groupId&gt; &lt;artifactId&gt;hazelcast-spring&lt;/artifactId&gt;&lt;/dependency&gt;Add our configuration file help us connect to Hazelcast.@Configurationpublic class HazelcastConfiguration { @Bean public Config hazelCastConfig() { Config config = new Config(); config.setInstanceName(\"hazelcast-instance\") .addMapConfig( new MapConfig() .setName(\"configuration\") .setMaxSizeConfig( new MaxSizeConfig(200, MaxSizeConfig.MaxSizePolicy.FREE_HEAP_SIZE)) .setEvictionPolicy(EvictionPolicy.LRU) //least recently used .setTimeToLiveSeconds(-1)); return config; }}Once configured we can autowire@Autowiredprivate HazelcastInstance hazelcastInstance;and use it to write and read from Hazelcast.Now to configure a simple spring boot mongodb app :Now to configure our traces we simply have to add this, obviously with different names for spring.application.name so we can distinguish between the two.spring.application.name=spring-cloud-hazelcastspring.zipkin.baseUrl=http://localhost:9411spring.sleuth.sampler.probability=1.0andspring.application.name=spring-cloud-mongodbspring.zipkin.baseUrl=http://localhost:9411spring.sleuth.sampler.probability=1.03. Comparing Hazelcast and MongoDBWe can access the write and read API for both Hazelcast and MongoDB with a simple curl requestFor mongodb:curl -X POST \\ http://localhost:8181/mongodb/api/write/1 \\ -H 'cache-control: no-cache' \\ -H 'content-type: application/json' \\ -d '{\"type\":\"testing\",\"to\":\"st.malike@gmail.com\",\"idField\":1}'curl -X POST \\ http://localhost:8181/mongodb/api/read/1 \\ -H 'cache-control: no-cache' \\ -H 'content-type: application/json' \\ -d '{\"type\":\"testing\",\"to\":\"st.malike@gmail.com\",\"idField\":1}'and Hazelcast curl -X POST \\ http://localhost:8080/hazelcast/api/write/1 \\ -H 'cache-control: no-cache' \\ -H 'content-type: application/json' \\ -d '{\"type\":\"testing\",\"to\":\"st.malike@gmail.com\",\"idField\":1}'curl -X POST \\ http://localhost:8080/hazelcast/api/read/1 \\ -H 'cache-control: no-cache' \\ -H 'content-type: application/json' \\ -d '{\"type\":\"testing\",\"to\":\"st.malike@gmail.com\",\"idField\":1}'Now we can head over to Zipkin to verify the results.Hazelcast Write took : **3.431 ms**MongoDB Write took : **417.952 ms**Hazelcast Read took : **2.218 ms** MongoDB Read took : **107.486 ms**As you can see the difference is quite clear.4. Use Cases for HazelcastCachingHazelcast has faster reads which makes it a good candidate for a cache. This architecture requires putting Hazelcast before a database or an API with immutable response.By providing in-memory access cache data, performance of the systems designed with Hazelcast will improve significantly.Speed Layer of Lambda Architecture and OLAP storeHazelcast can be used as the foundation of the speed layer of a Lambda System. The speed layer computes functions over recent data with low latency and mutates your real-time data stores directly. This can be used for real time analysis due to the fact that large volumes of data can easily be accessible from an in-memory grid of data fast for real time or near real-time analysis.This could also serve as a good store to build an OLAP unit on top of.These are just some of the few use cases for Hazelcast, you can find other use cases here as well. Source code also available on githubReferenceshttps://blog.hazelcast.com/" }, { "title": "Beginning Kubernetes and Istio Service Mesh for Cloud Native/Distributed Systems", "url": "/posts/K8s-101-And-Istio-Service-Mesh/", "categories": "devops, distributed, sre", "tags": "kubernetes, microservice", "date": "2018-10-11 00:00:00 +0200", "snippet": "1. Kubernetes [K8S]The Processes factor of 12 factors which means having stateless services, that can be easily scaled by deploying multiple instances of the same service. Deploying and management multiple instances of these stateless services can be a challenge if not organized properly. Coupled with features like load balancing, monitoring/health checks, replication, auto scaling and being able to roll updates with little overhead.This is where Kubernetes comes in. Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applicationsA Kubernetes setup consists of several parts some mandatory for the whole system to function.Componentshttps://kubernetes.io/docs/concepts/architecture/cloud-controller/Kubernetes Master or Master NodeThis is responsible for managing the cluster. This the cluster’s control plane, responsible for orchestrates the minions.kube-apiserver [Kubernetes Master]This is the part of the Kubernetes Master that is provides API for all the REST communication used to control the cluster.etcd storage [Kubernetes Master]Simple HA key-value store for storing configuration and cluster data.kube-scheduler [Kubernetes Master]This is responsible for deploying configured pods and services onto the nodes.kube-controller-manager [Kubernetes Master]This uses apiserver to watch the shared state of the cluster and makes corrective changes to the current state to change it to the desired one. That is to noticing and responding when nodes go down or maintaining the correct number of pods of cluster.cloud-controller-manager [Kubernetes Master]This is a new feature which runs controllers that interact with the underlying cloud providers. Allowing cloud vendors code and the Kubernetes core to evolve independent of each other.Kubernetes Minions or Worker NodeThese contains the pod and all necessary services to ensure the pods function properly.kubelet [Kubernetes Minions]An agent that runs on each node in the cluster. It gets the configuration of a pod from the kube-apiserver and ensures that the described containers are up and running.kube-proxy [Kubernetes Minions]It’s responsible for network routing. Basically a proxy.Container Runtime [Kubernetes Minions]The responsible for running containers. It supports Docker and other containers but most use cases are with docker.Other things to know although not shown in the diagram are :PodA pod is when a single or multiple containers are wrapped and abstracted so it could be deployed in Kubernetes. They normally don’t live long and in the coming section under deployments, you’ll se how pods can be created and deleted.ServiceThis is an abstraction on top of a number of pods, typically requiring to run a proxy on top, for other services to communicate with it via a Virtual IP address.This is where you can configure load balancing for your numerous pods and expose them via a service.Deployment ConfigurationPod and Services can be created separately but a deployment configuration combines pods and services with extra configuration to show how pods should be deployed, updated and monitored.These different ways to configure deployments on Kubernetes each has their pros and cons. There’s a link in the references that shows the pros and cons of each deployment.But toi summarize briefly:i. Recreate: This will end the old version and recreate the new one. Mostly suitable for dev environments or new uat environments. sample configii. Ramped: This type of deployment releases a new version one after the other by creating secondary replica sets of new pods and removing the old pods till the exact configuration sample configiii. Blue/Green: This involves deploying the new one alongside the old one. Labeled as “blue” and “green”. After testing the new one and seeing that it meets requirements, we switch traffic. sample configiv. Canary: This is almost like the Blue/Green Deployment but the main difference is you release the services to a set of users first. It’s easier to implement with service mesh like Istio and Linkerd.sample configSetting up KubernetesSome of the ways to run :a. Local-machinei. Minikubeis the recommended method for creating a local, single-node Kubernetes cluster for development and testing. Setup is completely automated and doesn’t require a cloud provider account.ii. Docker EdgeDocker for Mac 17.12 CE Edge includes a standalone Kubernetes server and client, as well as Docker CLI integration. The Kubernetes server runs locally within your Docker instance, is not configurable, and is a single-node cluster. However this is only for local testing.b. Hostedi. Google Kubernetes Engine offers managed Kubernetes clusters.ii. Amazon Elastic Container Service for Kubernetes offers managed Kubernetes service.iii. Azure Kubernetes Service offers managed Kubernetes clusters.Interfacing your Kubernetes Clusterkubectl :A cli tool to communicate with the API service and send commands to the master node.Web UI (Dashboard):A general purpose dashboard that allows users to manage and troubleshoot the cluster and it’s applications.2. Service MeshService meshes like Istio, Linkerd and Traefik do not come with Kubernetes out of the box and but I’ll like to take a minute to talk about Service Mesh and then we can see the role it plays in our kubernetes cluster.First of all, let’s start with the default service resource in Kubernetes. If you’ve read up to here you have a basic understanding of how the service resource works. With some of it benefits as Service discovery, Load balancing. Although all these please play vital roles there are certain fallbacks.Take for instance the LoadBalancing feature of a service resource that’s in front of 2 instances of a pod,A and B. Unfortunately Pod A is not performing very well the Service resource will still forward request to Pod A. A good service mesh will monitor the response rate’s of the pods and based on their response will know how to forward the response to the pods. With other functionalities like forwarding request based on http header, we are able to get a smart load balancer that can forward request based on latency. Using the http headers we can have the Canary type of deployment. For example deploy a pod for all users in a particular region on a particular device based on the http headers.Other advantages of service mesh include distributed tracing, circuit breaking. This is done with no development. This level of abstracting distributed tracing,circuit breaking from the microservice into the service mesh makes them easy to use aside it’s other benefits.For this project,we’ll be using Istio. Istio is built on top onf Envoy, which is used as the data plane or sidecar proxies to the pods. The other part, the control plane, configures envoy to route traffic, Istio-Auth for service-to-service auth and user-to-service auth and telemetry using Mixer. One coolfactor about Mixer is it’s support for different adapters. Eg: Prometheus, Datadog etc. This makes Istio very easy to use and for the preferable choice when it comes to Service Mesh.3. Putting it altogetherPutting everything together ,I’ll create a cluster with a java, go microservices and an html frontend.1. Building the docker imageIf you read my post on using Travis CI to set up a CI pipeline, I’ll use the same setup but with a different configuration file to build docker containers for the services which will be automatically pushed toDockerHub. Any CI pipeline that can get your service packaged as docker container will also work.2. Configuring the Pod ResourceIf you followed step 1. you’ll have a docker image on uploaded to your private repo or DockerHub in our case. We then configure the Container Runtime to pick the docker images as pods.Here’s a sample configuration explaining what it mean :apiVersion: v1kind: Podmetadata: name: cloudnative-java-service labels: app: cloudnative-java-service version: cloudnative-java-servicespec: containers: - image: stmalike/java-service name: cloudnative-java-service ports: - containerPort: 8080kind : Type of resource.name : Name for resourcelabels.app : Label for podspec : For a pod config, this accepts an array of containers to be run in a single pod.image : Details about the image to be run in pod.resources : CPU and memory resource limits for podThis is a simple example to configure your pod but you can do moreNow the Container Runtime in the kubernetes cluster has 3 pods configured to pick docker images from DockerHub.frontend pod,java microservice pod and go microservice pod.Finally we register our pods:kubectl create -f ./java-service/java-service-pod.yamlNow let’s verify our pods were created successfully.kubectl get pods --show-labels3. Configuring Config MapAt this point we’ve successfully gotten our microservices running in a pod. Next step we want a centralized place and secured place for our app configurations.In a previous post I shared the benefits of working with a configuration management service. The post highlighted how it could be done with Spring Cloud Config. Kubernetes also has a configuration servive we can use to achieve that called ConfigMap.We can create a config server with the command below :kubectl create configmap cloudnative-config --from-file=config/application.propertiesNow that we’ve created a config, we need to configure the pods to know about ConfigMap to enable them load configurations.volumeMounts: - name: cloudnative-config mountPath: \"/config\" readOnly: true volumes: - name: cloudnative-config configMap: name: cloudnative-config items: - key: application.properties path: application.propertiesThe configuration simply mounts a drive /config in the java-service docker container with the configuration file. Fortunately we just need a file application.properties in a folder named as config where our spring boot jar is for it to pick it automatically. The loading of thsi properties overrides the default configuration we have for instance.name to be Two.By this we’ve configured the java-service pod to load instance name from the configuration file. This is to help us run 2 different pods of the java-service with different property value for instance.name.4. Configuring the Service ResourceAt this stage, we’ve successfully configured the pods with our docker containers. We’ll need to create an API Gateway to access the pods. This is where the service resource comes in.We’ll need to configure our service resource to work as a Load Balancer, when there are multiple instances of the same pod and also serve as the main entry point or api-gateway for pods of the same type. Kubernetes uses the label configuration to classify pods this helps the load balance know which of the pods are the same.apiVersion: v1kind: Servicemetadata: name: cloudnative-java-service-lbspec: type: LoadBalancer ports: - port: 80 protocol: TCP targetPort: 80 selector: app: cloudnative-java-servicekind : Type of resource.name : Name for resourceport: Port of the service.protocol: Protocol of the communication.targetPort: The port exposed in the pod.selector.app: Pod where requests should be forwarded.Read more about servicesTo create the service execute the following command:kubectl create -f ./java-service/java-service-lb.yamlNow let’s verify that the load balancers are up and running.Yo can also get this from the terminal using :kubectl describe svc java-service-lbTo confirm if our load balancer really works, we can create to pods of the java service, with one pod configured with One as instance.name and the other Two. By sending request to the LoadBalancer we can confirm if the request is routed to pods.instance oneinstance twoAs you can see you our LoadBalancer has successfully routed request to both pod instances. To be frank I had to refresh the page like a million times before seeing the second image.Creating Pods and connecting them with Services is all cool and all but there are certain things that are lacking. For example, the pod instances can go down and will require manual effort to bring them up.When we delete all the pods, probably to update them, our load balancer will return an empty response because the pods are not governed by a deployment config. Imagine this pod is responsible for accepting payments on a highly active application.The next section highlights some of the awesome ways we can deploy, monitor, update and easily rollback deployments using the deployment configuration with benefits the cluster will lack if we created pods and services instead of deployments.5. Ramped DeploymentAs described in Deployment Section, Ramped Deployment ensures new versions one after the other by creating secondary replica sets of new pods and removing the old pods till the exact configuration is met. Below is a sample configuration we are using in sample project.apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: cloudnative-java-service-rampedspec: replicas: 2 minReadySeconds: 5 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 template: metadata: labels: app: cloudnative-java-service spec: containers: - image: stmalike/java-service imagePullPolicy: Always name: cloudnative-java-service ports: - containerPort: 8080 livenessProbe: httpGet: path: /health port: http initialDelaySeconds: 40 timeoutSeconds: 1 periodSeconds: 15 readinessProbe: httpGet: path: /health port: http initialDelaySeconds: 40 timeoutSeconds: 1 periodSeconds: 15 volumeMounts: - name: cloudnative-config mountPath: \"/config\" readOnly: true volumes: - name: cloudnative-config configMap: name: cloudnative-config items: - key: application.properties path: application.propertieskind : Type of resource, Deploymentspec.replicas : Number of instances of the podspec.strategy.type : Type of Deployment Strategy. Note RollingUpdate is also known Rampedspec.strategy.rollingUpdate : Ramped deployment configuration which specifies the maximum number of pods that need to unavailable at a time and number of pods to be added per deployment(maxSurge)template: Template to create new pods with label configurationkubectl apply -f java-service/java-service-ramped-deployment.yamlWe can see the status on the dashboardOr kubectl with command :kubectl rollout status deployment cloudnative-java-service-rampedTo verify, you can access the service http://127.0.0.1:9090/javaservice and check details of the deployment from the dashboard or kubectl with :kubectl describe deployments cloudnative-java-service-rampedScaling Ramped DeploymentKubernetes supports automatic vertical scaling of pods but for the purpose of making this article short, it will be dealt in another post.For manual vertical scaling, you can easily do it from the dashboard or the deployment configuration.i. Update replica size to 3ii. New pod startediii. New pod runningor by just changing this part of our deployment configuration and run this command:spec: replicas: 3 kubectl apply -f java-service/java-service-ramped-deployment.yaml --recordOne thing you’ll notice after increasing the replicas to 3 is although we had only 2 pods initially by increasing the replicas to 3 Kubernetes notices that our pods running do not match and automatically starts 1 extra pod. That’s all it takes. For a manual process this is pretty simple so you can imagine how the automatic vertical scaling will work.No Downtime Update of Pods in Ramped DeploymentTo show how this will work, I created a docekr image of the java-service with a slight update to the json retunred and taged it as UPDATE2_0.This is the update we want to deploy, we start by changing the config to point to the new imagespec: containers: - image: stmalike/java-service:UPDATE2_0 imagePullPolicy: Always name: cloudnative-java-serviceand then execute kubectl apply -f java-service/java-service-ramped-deployment.yaml --recordYou can continously refresh the link http://127.0.0.1:9090/javaservice, you’ll notice that the content changed without any downtime with the load balancer.The stages of how the updates happened can be visualized on the dashboard as followsi. Brings down one pod ,because “maxUnavailable” is 1 in deployment configii. Adds new updated pod as secondary ,because “maxSurge” is 1 in deployment configiii. Removed old versions and voila we have our clusterand as usual this can also be monitored on the terminal using :kubectl rollout status deployment cloudnative-java-service-rampedDuring the whole process if you pinged the load balancer, you’ll continue getting a response till it finally updates. NO DOWN TIME.RollingBack Updates in Ramped DeploymentAs if updates are not cool enough, rolling back updates is even simpler, Our new container has a bug, the json returned by the service, http://127.0.0.1:9090/javaservice , can’t be parse.first get your deployment historykubectl rollout history deployment cloudnative-java-service-rampedand then just revert to the previous revision number which is 1 in this case.kubectl rollout undo deployment cloudnative-java-service-ramped --to-revision=1…and again, the roll back is done with no downtime.So we understand how the Ramped deployment works. Now we can look at another type of deployment.6. Canary Deployment with IstioThe Ramped is cool and all till we need to deploy a specific service for our customer in a particular region or using a particular channel ,for example iOS mobile. This is where the Canary deployment is useful.I also wanted to use Istio in this example to show how awesome Service Meshes are and what vital roles they can play in our cluster.Install Istio and after enable automatic sidecar injection for our default namespace. This just means every pod deployed in the default namespace will have will have Istio Sidecar.kubectl label namespace default istio-injection=enabledor manuallykube-inject -f java-service/java-service-canary-istio-deployment.yaml | kubectl apply -f -When we check the pods with bash kubectl get pods it will confirm the Istio side-car proxy,Envoy, was also installed into our pod as well. The pods now show 2 items in each pod.Istio deployment configuration which will route all request with header channel as mobile to instance 2 of our java microservice but everything else will go to instance 1.There 2 deployment files for the java-service with different tags, latest and UPDATE2_0. UPDATE2_0 will handle all mobile requests and the other for web request.So this is the initial deployment and service configuration for the applicationapiVersion: apps/v1beta1kind: Deploymentmetadata: name: cloudnative-java-service-canary labels: app: cloudnative-java-servicespec: replicas: 1 template: metadata: labels: app: cloudnative-java-service version: cloudnative-java-service spec: containers: - image: stmalike/java-service imagePullPolicy: IfNotPresent name: cloudnative-java-service ports: - containerPort: 8080 volumeMounts: - name: cloudnative-config mountPath: \"/config\" readOnly: true volumes: - name: cloudnative-config configMap: name: cloudnative-config items: - key: application.properties path: application.properties---apiVersion: v1kind: Servicemetadata: name: java-service-lbspec: type: LoadBalancer ports: - port: 9090 protocol: TCP targetPort: 8080 selector: app: cloudnative-java-serviceThis has to be updated to version UPDATE2_0 but just for users on mobile first. Based on the performance we can enable it for our users on web.Being able control the routing in Istio will require 3 configuration a Gateway, VirtualService and DestinationRule.apiVersion: networking.istio.io/v1alpha3kind: Gatewaymetadata: name: java-service-istio-lbspec: selector: istio: ingressgateway servers: - port: number: 9191 name: http protocol: HTTP hosts: - \"*\"---apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: java-service-istio-vsspec: hosts: - \"*\" gateways: - java-service-istio-lb http: - match: - headers: channel: exact: mobile route: - destination: host: java-service-istio-rule subset: mobile-route - route: - destination: host: java-service-istio-rule subset: web-route---apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata: name: java-service-istio-rulespec: host: \"*\" subsets: - name: mobile-route labels: version: cloudnative-java-service-mobile - name: web-route labels: version: cloudnative-java-servicekubectl create -f java-service/java-service-canary-istio-deployment.yamlGateway acts as a load balancer which handling requests and their response.VirtualService which is bound to a gateway to controls forwarding of the request that comes to the gateway.DestinationRule defines the routing policies.We have different types of routing policies in Istio and it’s not just restricted to headers present in request.This makes Istio smarter load balancer. It can forward request based on performance of the recieving pods, or configure fixed percentage of traffic distributed to multiple pods based on their specs, forward based on user or user location and other routing techniques. It makes Istio for me ..A MUST HAVE in any kubernetes cluster.One other HUGE advantage of using Istio is the out-of-the-box added benefits. Which requires little configuration and no coding to use.1. canary mobile route2. canary web routeOnce everything is perfect for our mobile users we can easily upgrade the pod in the web deployment configuration java-service-canary-istio-deployment.yaml to complete Canary deployment using Istio spec: containers: - stmalike/java-service:UPDATE2_0 imagePullPolicy: IfNotPresent name: cloudnative-java-service-canaryCollecting and Visualizing MetricsIstio and Envoy proxy automatically collects metrics and this requires no development. These metrics are forwarded to Mixer which supports multiple adapters.Based on these metrics we can create rich dashboards to understand and monitor performance of the cluster.To visualize the metrics in Prometheus, we runkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=prometheus -o jsonpath='{.items[0].metadata.name}') 9090:9090 &amp;By forwarding the port we can access the dashboard on http://localhost:9090There is also a Grafana with Prometheus as datasource, again we port-forwardkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath='{.items[0].metadata.name}') 3000:3000 &amp;And then access it http://localhost:3000/dashboard/db/istio-dashboardDistributed TracingIn a previous post I talked about Distributed Tracing using Opencensus, Zipkin, ELK and Spring Cloud Sleuth. It all required some level of coding to get it done. But with this commandkubectl port-forward -n istio-system $(kubectl get pod -n istio-system -l app=jaeger -o jsonpath='{.items[0].metadata.name}') 16686:16686 &amp;We can visualize traces in Jaeger here .There are other benefits all with Kubernetes and Istio combination. But I hope this post can be used as foundation to build upon that. In my next post, I’ll move the current setup to the cloud. Find source codes hereREFERENCEShttps://12factor.net/https://kubernetes.io/docs/setup/pick-right-solution/https://kubernetes.io/docs/concepts/overview/components/https://container-solutions.com/kubernetes-deployment-strategies/" }, { "title": "Build, Release, Dockerize and Run with Github, Travis CI and DockerHub", "url": "/posts/Build-Release-Dockerize-Run-With-Travis-CI/", "categories": "devops, sre", "tags": "docker, kubernetes, distributed, go, java", "date": "2018-09-11 00:00:00 +0200", "snippet": "If you’ve read my previous post, Configuration Management for Microservices, I created two microservices one in Golang and the other in Java. Using the two, I’ll want to show how we can use Travis CI for the Build, Release and Run phase.In a later article we’ll use our docker images with Kubernetes.1. Building Docker Images From Github for Go and Java ReposTo build a docker image for a our source codes we’ll create Dockerfile.The Dockerfile is a set of instructions to build and properly describe a docker image.Here’s what some of the instructions mean :Sample Dockerfile for JavaFROM openjdk:8VOLUME /tmpADD target/config-server-1.0-SNAPSHOT.jar config-server.jarRUN sh -c 'touch /config-server.jar'ENV JAVA_OPTS=\"-Xms128m -Xmx256m\"EXPOSE 8888ENTRYPOINT [ \"sh\", \"-c\", \"java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -Dspring.profiles.active=docker -jar /config-server.jar\" ]The Go Dockerfile is a multi-stage approach to build light weight images explained into details here.FROM: Image with its tag as build-base. This is mostly alpine linux because it’s light and allows debugging of containers. There are other options as well like scratch.RUN: Is used to execute a shell command-line within the target system.COPY: Used to copy files from the local file-system to target image.ADD: Used to also copy files like COPY but with addition features.ENTRYPOINT : Entrypoint sets the command and parameters that will be executed first in target system when it starts.ENV : Used to define environment variables used in the target host.CMD: Pass arguments to the ENTRYPOINT in the target system.You can check out the docker documentation to read more on it and this on how to build build small size docker containers for Go2. Setting up our CI server : Travis CI.“Travis CI a hosted, distributed continuous integration service used to build and test software projects hosted at GitHub”Like the Dockerfile is a set of instructions to build a docker image, .travis.yml also contains a set of instructions for Travis CI to build and test the source hosted on Github.In our case we’ll use Travis CI to test and compile our source and then package a docker image that can easily be run. Travis CI doesn’t store the docker images so we need to push to a docker repository. This can be a private one or a public one. For this example we’ll use the public DockerHub.To build and test our project,which uses maven, we’ll add this.script: cd $SERVICE_DIR &amp;&amp; mvn clean verifyThe maven goal test is intentionally left our because Travis CI does that by default once we specify java, I’m using the script because thats way let Travis CI building sub directories projects.This works because I’ve specified the sub directories as environment variables.env: global: - SERVICE_DIR=config-server - SERVICE_DIR=message-summaryAfter successfully building the source we’ll need to package it using the docker configuration, Dockerfile. But first we’ll have to enable Docker for Travis CIservices: - dockerI wrote a bash script to handle the building and the uploading of the images. It uses the known docker build, docker tag and docker push commands to complete this task.First of Travis CI, would need access to upload the packaged images. Travis CI has a simple system for not storing passwords in plain text which requires installing a travis gem sudo gem install travis. Encrypt your passwords and add them to your travis config file by cd-ing into the project folder then travis encrypt VARIABLE=\"secret-to-be-encrypted\", confirm the repository name, it should return an encrypted text which you can add to your .travis.ymlenv: global: - secure: \"encrypted-dockerhub-username-1234\" - secure: \"encrypted-dockerhub-password-1234\"For more details on writing your travis configuration to read more on itOK, now that we have a Docker image that is built by Travis, how do we use it? Well, we want to push the image to a Docker Registry for storage. Users can then pull the image from the registry to the machine where they will run the image.Complete .travis.yml would look like this for a Java applicationsudo: requiredlanguage: javajdk: - oraclejdk8before_script: - mongo go_kafka_alert --eval 'db.createUser({user:\"travis\",pwd:\"test\",roles:[\"readWrite\"]});' services:- mongodb- docker env: global: - secure: \"encrypted-dockerhub-username-1234\" - secure: \"encrypted-dockerhub-password-1234\" - SERVICE_DIR=config-server - SERVICE_DIR=message-summarybefore_install: - chmod +x ./.travis/pushimage.shscript: cd $SERVICE_DIR &amp;&amp; mvn clean verifyafter_success: - bash ../.travis/pushimage.sh config-server - bash ../.travis/pushimage.sh message-summaryCheck .travis.yml for the travis config for our Go applicationSummary so we successfully setup an automated process using Travis CI, Dockerfile configurations to build an image and then successfully uploaded to DockerHub. Although the title is “Build, Release, Dockerize and Run with Github, Travis CI and DockerHub” we’re yet to actually run our service. This would be continued in the next article with other things we can introduce in the pipeline to produce efficient results.REFERENCEShttps://12factor.net/https://medium.com/@pierreprinetti/the-go-dockerfile-d5d43af9ee3c" }, { "title": "To Contain Or Not To Contain", "url": "/posts/To-Contain-Or-Not-To-Contain/", "categories": "devops, sre", "tags": "docker, kubernetes, distributed, go, java", "date": "2018-09-10 00:00:00 +0200", "snippet": "So some of the known reasons why some people have talked to me about not adopting containers. So this my attempt to convince them based on the concerns raised and few use cases where I won’t suggest using containers.Here goes :1. Why put a “container” in a container?First of all it seemed like a convenient way to shift the issues from go builds or jars to containers. “It works on my machine” to “It works in my container”.Also an extra abstraction that wasn’t needed, what’s really the difference between eg between : java -jar for jar files and docker run for docker containers. In both cases a jar, for example would have all dependencies packaged and can be run with a single command. Especially when we had all of the builds “wrapped” in shell scripts which we could start with service service_name start or systemctl start service_name.but,It’s more than that, containers providei. Uniform abstraction, in a microservice architecture where you might have services developed in different languages, each tackling specific parts of the ecosystem one will use a container to provide a uniform abstraction independent of coding language.ii. Processes, one of the 12 factors for Cloud native apps indicates that an app is executed in the execution environment as one or more stateless processes. Containers help in packing and deployment of various microservices and only exports the services over a port, which is also one of the factors.What a container brings on board which the other options don’t is, it’s easier to start/stop or automate both based on requirements. Again this is because of the uniform abstraction. This is done irrespective of the OS and dependencies required by the service. Since all the all the dependencies can be packaged in the container.2. Security and MaintainabilityFewer the merrier : Fewer dependencies, fewer security issues, fewer dependencies to deploy your service.For example the ELK docker containers has lots of images how would you be sure of the integrity of any of the images you want to use. To be certain it wasn’t compromised.but,i. Build, Release and Run indicates we strictly separate build and run stages to basically transform our codebase through a non-development process,usually automated, by fetching code dependencies to obtain a executable binaries. This is called the Build stage, closely followed by the Release which tags binary with the right configurations and settings for next stage Run which just means executing the application.This set of processes could be mundane if done manually, a container makes it worse. The end product of the this step, the container, could use unverified dependencies which could be security concerns. Just us devs need to verify and confirm dependencies used in source codes, the same process needs to be done for containers.Most of CI platforms support containers in their process, to the point of automation as well.Some (docker to be specific) of the best practices for running containers in production have been highlighted there, I left out some of the “obvious” ones.In my next post we’ll use Github to build a CI pipeline for a Build, Release and Run CI pipeline with Docker to highlight.“Not To Contain”1. Backing services?IMO containers are cool but not everything needs to be in container. This is not because of problems with docker or if there are no means to do it.Here’s why :Containers are great for running databases in a dev environments but running backing services on production eg: dbs, messaging queues , etc… in a container just a no no. You’re better of using a managed services provided by cloud services. If you really have to self-host such services in a reliable fashion, you’re in for a lot of work and learning which is by far still a better alternative to containers like docker.2. “Microservic-ing” a monolith by putting it in a container?There was a time we had a long discussion in office on how micro a microservice needs to be, is this the individual REST points each in a container or a bunch of like-minded REST services in a single container. There are many school of thoughts on this but then again, a small software company in Ghana wiling to adopt microservices is not Netflix or Google to split services to minute end points in containers especially when you’re now about to go on production. It doesn’t scale. Imagine a team of 3 devs working on 100 microservices with no automation or tools to support it.A microservice, to me, is an organisms which needs to evolve, change based on lots of measurements, monitoring and how users interact with it. It’s easier if you have a monolith from the onset so you can tell for instance Signup Service would need to scale and not Login Service so let’s decompose the Signup Service into a separate container to exist on its own for the start, which means it’s easier to scale etc. It’s not just breaking it up, it needs to abide by the 12 factors as well. It shouldn’t be tightly coupled with the other services.Modernize Traditional Applications is another toolkit by docker to help migrate traditional applications.REFERENCEShttps://12factor.net/https://sysdig.com/blog/7-docker-security-vulnerabilities/https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#create-ephemeral-containers" }, { "title": "Gitlab CI/CD Pipeline for a Maven Project", "url": "/posts/Gitlab-CI-CD-Pipeline/", "categories": "devops, sre", "tags": "devops, ci, cd, gitlab", "date": "2018-08-30 00:00:00 +0200", "snippet": "What is CI/CD?“Continuous integration focuses on blending the work products of individual developers together into a repository. Often, this is done several times each day, and the primary purpose is to enable early detection of integration bugs, which should eventually result in tighter cohesion and more development collaboration.”“The aim of continuous delivery is to minimize the friction points that are inherent in the deployment or release processes. Typically, the implementation involves automating each of the steps for build deployments such that a safe code release can be done—ideally—at any moment in time. Continuous deployment is a higher degree of automation, in which a build/deployment occurs automatically whenever a major change is made to the code.”Gitlab CI Agent is GitLab Continuous Integration &amp; DeploymentGitLab has integrated CI/CD pipelines to build, test, deploy, and monitor your code.To successfully create a CI/CD pipeline using Gitlab you’ll need a couple of things :1. Pipline on GitlabCreate your CI/CD pipeline on Gitlab or a self hosted gitlab as well.The pipeline,which is basically a set of instructions should be created in a file .gitlab-ci.yml placed in your project root. Check this to write your own but a simple pipeline would look something like this# These are the default stages.# You don't need to explicitly define them.# But you could define any stages you want.stages: - build - test - deploy# This is the name of the job.# You can choose it freely.maven_build: # A job is always executed within a stage. # If no stage is set, it defaults to 'test'. stage: test # Since we require Maven for this job, # we can restrict the job to runners with a certain tag. # Of course, it is our duty to actually configure a runner # with the tag 'maven' and a working maven installation tags: - maven - java - ubuntu # Here you can execute arbitrate terminal commands. # If any of the commands returns a non zero exit code the job fails. script: - echo \"Maven build started\" - mvn verify2. Setup Host serverInstall mvn and the necessary dependencies to run your application. Then install the gitlab runner which is also available for platforms like Linux, MacOS, K8s and Windows.If you’re too careful and would want to test if it works you can use Docker no need to pay for a new server.i. Verify your installation$ gitlab-runner listListing configured runners ConfigFile=/{HOME}/.gitlab-runner/config.tomlThis is expected because we do not have nay configured pipelinesii. Register a CI/CD pipelineRegister pipeline with :gitlab-runner registerYou’ll have to fill the prompt with the right parametersPlease enter the gitlab-ci coordinator URL (e.g. https://gitlab.com/):https://gitlab.com/Please enter the gitlab-ci token for this runner:token1234Please enter the gitlab-ci description for this runner:[$]: maven-demo-appPlease enter the gitlab-ci tags for this runner (comma separated):maven, java, ubuntuWhether to run untagged builds [true/false]:[false]:Whether to lock Runner to current project [true/false]:[false]: trueRegistering runner... succeeded runner=random_Q3_i4Please enter the executor: docker-ssh, parallels, ssh, docker+machine, docker-ssh+machine, docker, shell, virtualbox, kubernetes:shellNote:gitlab-ci:gitlab-ci tags is a discrimintor with which we can tell the runner to either deploy the build or skip it. With this example the tags are maven, java, maven and by setting the runner not to run untagged builds we’ve essentially turn the discriminator on. We assigned the tag to our build in the .gitlab-ci.yml file with this :tags: - mavenGitlab runner has different executors for running the build, the executor for the maven is shell.You can also configure Gitlab to pass custom environment variables. I prefer not to do this though. Configurations are always managedby a configuration server.iii. Running the BuildNow the Runner is ready we can test by pushing a commit to your GitLab. Gitlab automatically triggers the pipeline once the tags match.Each commit is represent differently on Gitlab with test results as well details on whether it was deployed on target host.There’s also information on when it was executedREFERENCEShttps://docs.gitlab.com/runner/" }, { "title": "Using Elasticsearch as an OLAP Cube", "url": "/posts/OLAP-Cube-With-Elasticsearch/", "categories": "foss", "tags": "olap, elasticsearch, report, analytics", "date": "2018-08-09 00:00:00 +0200", "snippet": "What’s an OLAPOnline Analytical Processing is the description of any technology that can help us to answer complex queries based on data stored in data warehouse, normally large volumes and across multiple sources. The data is structured such that complex queries would return results faster. This is purposely for decision making.It supports complex aggregation and filters to enable faster decision making about future data and insight about past data with advance analysis. Performance of an OLAP is measured in how fast it returns aggregated queries.Data written in OLAP systems are hardly updated, in most cases it’s not. This is because it would be an expensive query. So building an OLAP cube requires most if not all the variables carefully thought out.For a system to be used as an OLAP there are certain characteristics required.1. Multidimensional data analysisWhatever OLAP technology is being used, it must provide a multidimensional conceptual view of the data. It is because of the multidimensional view of data that we often refer to the data as a cube. A dimension often has hierarchies that show parent child relationships between the members of a dimension. The multidimensional structure shouldallow such hierarchies. This is one of the distinctive characteristics of OLAP.With multidimensional analysis, data can be processed and viewed as part of a multidimensional structure enabling decision makers to view business data as data related related to other business data over a period of time.2. Complex aggregationsThe previous point describes how the data is stored with this step we can create multiple data aggregation levels, filter data and drill down across different dimensions and aggregation levels for different purposes. We can view the data in the form of graphs and charts or drill downs in tabular forms over different periods like bi-weekly, monthly, quarterly or weekly.3. Business intelligence tool to visualizeThe abstraction of running and visualizing all complex aggregations into simple user friendly views is also of great importance.End users shouldn’t know about the complex queries needed to get drill-downs. Most of the end users of OLAP services are business folks who would want the data to make fast decisions.I can’t talk about OLAP without mentioning OLTP, to read about the main differences between OLAP and OLTP, check this out.Using Elasticsearch as an OLAP CubeWhy is Elasticsearch a strong candidate to be used as OLAP based on our understanding of OLAP?1. How Elasticsearch supports multidimensional data analysisElasticsearch supports document stores,JSON, which we can model in any way we want. With support for REST, we can design any complex data models and write them in any programming language. Elasticsearch also has a bulk insert api.2. How Elasticsearch supports complex aggregationsElasticsearch is based on Lucene and with the JSON store we can quickly aggregate and filter the data using possible combinations. Having used in production for a financial institution we could aggregate over five years ,~180 million data very fast to enable the decision makers gather needed data over the raw data.3. How Elasticsearch supports business intelligence tool to visualizeElasticsearch has a business analytics solution which is based on Elasticsearch and Kibana, all part of the Elastic Stack.Kibana already support reports but it’s use case is limited to data thats only on the view,HTML page, and does not give drill-down of data returned in the charts. Due to this limitation of generating reports with the analytics and reports of Elasticsearch and Kibana, I wrote a plugin Elasticsearch Report Engine to help generate more details reports and drill downs we see in Kibana.This plugin supports CSV,PDF and HTML formats.How it worksOnce this plugin is installed into elasticsearch search,it exposes the url http://localhost:9200/_generate, you can run queries on your cluster with the right parameters it would return PDF,HTML or CSV file.This version works on Elasticsearch 5.6.10Installsudo bin/elasticsearch-plugin install https://github.com/malike/elasticsearch-report-engine/releases/download/5.6.10/st.malike.elasticsearch.report.engine-5.6.10.zipGrant permissionFolder StructureCreate folders templates and reports in ES_HOME. Store your *.jasper and *.jrxmlfiles in the templates folder and pass the templateName as the template (with the right extension) parameter for HTML and PDF reports.READ MOREIf you have any problems using the plugin, kindly let me know orsend a PRREFERENCEShttp://datawarehouse4u.info/OLTP-vs-OLAP.htmlhttp://www.myreadingroom.co.in/notes-and-studymaterial/65-dbms/563-olap-and-its-characteristics.html" }, { "title": "Configuration Management for Microservices", "url": "/posts/Configuration-Management-For-Microservices-And-Distributed-Systems/", "categories": "distributed, sre, foss", "tags": "distributed, configuration, zookeeper, elasticsearch, microservice", "date": "2018-06-02 00:00:00 +0200", "snippet": "Centralized configuration management is one of the major requirements in a microservice system. Although one of the majorproponents of a microservice system is decentralized services, certain parts still need to be centralized to easedeployment pains.Centralized configuration is simply a main location where all microservices can pick their specific configurations to help them run perfectly.Some of the well known options are :1. etcd“etcd is a distributed reliable key-value store for the most critical data of a distributed system, with a focus on being:Simple: well-defined, user-facing API (gRPC)Secure: automatic TLS with optional client cert authenticationFast: benchmarked 10,000 writes/secReliable: properly distributed using Raft”When researching on using etcd as config server, I came across these resources this, implemented in java and also this in python .2. Zookeeper“ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. All of these kinds of services are used in some form or another by distributed applications”This is supported in Spring. If you use Apache Kafka, you’ll know how it relies onZookeeper’s high availability and fault tolerant for storing and retrieval of K,V pair of topic and consumer configurations.I found this when researching on using Zookeeper with Go. It’s quite old but with very good explanation.Netflix’s Apache Curator is java client for Zookeeper which extremely helpful with especially when not using the Spring stack.3. ConsulConsul is not just a K,V store, one underlying principle of Config Servers. It has Service Discovery, Service Health Monitoring as well as other functionalities which makes it the preferred choice. With it’s well documented apis and sdks with examples in different languages and frameworks it would be the best option. But the simplicity of the next option is what makes this post not about Consul.…and finally 4. Spring Cloud ConfigSince this post is about Spring Cloud Config, let’s dive in :It’s best to explain codes, if you follow the link you’ll see a project where by a golang and a java application worked with a Spring Cloud Config to retrieve configurations. The golang project is an already existing project go-kafka-alert.1. Manual Configuration RefreshAside from just serving as a K,V store to store and retrieve configurations, a Config Server should have in place systems to help update or refresh configurations in services after updates. In a Spring Boot/Cloud application connecting to a config server,this can be achieved by doing the following:a. Location of config Server, by adding this to property file. This informs the Spring Boot/Cloud application to override default properties by checking the Config Server for properties.spring.cloud.config.uri=http://localhost:8888It uses this convention http://localhost:8888/{spring.application.name}-{profile}.yml. Where spring.application.name is the name of the service configured in your spring application.b. Secondly using @RefreshScope annotation to distinguish properties to be updated after refresh and those that should not. In our sample applicationc. LastlyHit the /refresh endpoint of the service for our service to reload the new configurations.NB: You’ll need org.springframework.boot:spring-boot-starter-actuator as part of your project to expose the refresh endpointHow would you replicate a similar thing in a Golang application,2. Automatic Configuration RefreshWe’ve seen how the manual configuration update works. But it could be better to not involve manual processes.One advantage of this is,it reduces the amount of work to done with after configuration updated and also helps us use Spring Cloud Config in microservices or distributed systems not written using the Spring stack.This would require a messaging queue,Apache Kafka or RabbitMQ. Redis is also a supported by Spring Cloud Config. It also requires a git webhook in Github,Bitbucket or Gitlab to tell the config server if any change is pushed, that is for configuration files hosted on Git. The messaging queue enables services subscribed to see the configurations files updated.To get this working we’ll need to add dependencies for spring-cloud-config-monitor and spring-cloud-starter-bus-kafka( you can use either redis or rabbit bus dependency)spring-cloud-config-monitor enables the end point /monitor in our config server which we can then add as webhook to our configuration repositoryNB: Although this image shows how-to in github,it can also be done in gitlab and bitbucketAnytime there’s a change in this repo it’s pushed to the Config Server. For configuration file storedon the file system Spring Cloud Config automatically picks the changes so you won’t require any webhook.A sample Cloud Bus configuration in Spring Config Server, detailing where it should pick the configuration as well Kafka configuration to push events to. spring: application: name: config-server cloud: config: server: monitor: github: enabled: true health: enabled: true git: uri: https://github.com/malike/centralized-configuration.git force-pull: true username: username password: password bus: enabled: true refresh: enabled: true stream: kafka: binder: zkNodes: localhost:2181 brokers: localhost:9092If our service is a Spring Boot/Cloud application we would need just one other configurationfor it to work out of the box but for a Go (or a non Spring Boot/Cloud ) application also using the Config Serverwe’ll need to be subscribed to the topic springCloudBus and update the config anytime there’s anupdate the configuration file in used.sample message from in kafka after updating the go-kafka-alert-production we can use to trigger configuration reload3. Configuration Format SupportFormats supported by Spring Cloud Config are JSON, YML and Java properties. The addded advantage of Spring Cloud Configis any configuration file served as either format can be accessed in all available formats. For example a config filethat’s served as application-message-summary-uat.properties can be accessed by using application-message-summary-uat.propertiesor application-message-summary-uat.json or application-message-summary-uat.yml.In the sample configurations there’s a yml configurationConfiguration files can be loaded from the file system or git with a simple configuration or relational databases with extra configuration and db setup.this for file systemspring.cloud.config.server.git.uri=${HOME}/Desktop/configor this for gitspring.cloud.config.server.git.uri=https://github.com/malike/centralized-configuration.git4. HTTP Support For Retrieving ConfigurationsAlthough Spring Cloud Config is a java based application it can work with a variety of languages due to it’s HTTP API.http://localhost:8888/{application}/{profile}[/{label} http://localhost:8888/{application}-{profile}.{yml|json|properties}http://localhost:8888/{label}/{application}-{profile}.{yml|json|properties}http://localhost:8888/{application}-{profile}.{yml|json|properties}http://localhost:8888/{label}/{application}-{profile}.{yml|json|properties}NB: Base url is http://localhost:8888 because thats the configuration in our config serverFor our sample app, with configuration names message-summary and go-kafka-alert we’ll create config files with profiles for uatand production as message-summary-production.properties or message-summary-uat.properties. As I mentioned earlier the format doesn’t really matter once it’s supported we can access with with any extension meaning we can access our yml configuration files as json or properties.5. SecuritySpring Boot/Cloud apps can be secured simple by adding spring-boot-starter-security and with the right configuration we can enable basic authentication for the Config Server.We can also take it a step further to restrict passwords and sensitive data in the configuration. This would prevent anyone who has access to the configuration files from seeing our database credentials in plain text. Spring Cloud Config supports encrypting and decrypting properties. You can read more hereThis would require you to install Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy File which is not part of the JVM by default. By pre-pending passwords and sensitive properties with {cipher} and in quotes. Sensitive properties would be encrypted in git but would be decrypted on requests by services. Spring Cloud Config also comes with two end points to help in encrypting and decrypting properties.To enable creating your key and adding it to your config server.There many pros and cons of using Spring Cloud Config, I think I’ve highlighted the pros a lot in this post but some of the major cons associated with this the Config Server becoming an app on it’s own that needs to be managed and monitored like the microservices or distributed systems are not already enough and the fact that it going down can force services to either not start or or rely on fallback configurations but then again these same points maybe seen as pros in other infrastructures.I do hope though that you’ve understood how to use Spring Cloud Config to serve configurations in a microservice or distributed system. Check out codes on githuband configuration files also hereREFERENCEShttps://cloud.spring.io/spring-cloud-config/single/spring-cloud-config.html" }, { "title": "Distributed Tracing with Spring Cloud Sleuth and Opencensus", "url": "/posts/Distributed-Tracing-With-Spring-Cloud-Sleuth-And-Opencensus/", "categories": "distributed, sre, foss", "tags": "distributed, tracing, opencensus, elasticsearch, microservice", "date": "2018-04-09 00:00:00 +0200", "snippet": "1. What’s Distributed Tracing?Assume you’ve designed your application using a microservice architecture and for a feature like signup you have:a. service that creates the user in the database, b. …another service that send an event to a messaging queue, c. and finally a service to send an email to the user reacting to the event created in the messaging queue.FYI this is just signup. (BTW I don’t recommend starting a project like this unless it’s an already existing project you’ve decided to break apart but I’ll leave that for another day).Monitoring can be a problem since requests can propagate between multiple services, probably running on different containers/hosts. In a “pure” microservice system the services might not even be sharing resources like database. How then can we explain or know that although Kofi Mensah’s account details were successfully created by the signup service, the emailing service failed to send an email to Kofi Mensah without having to grep through multiple log files across different hosts. With this particular example it’s not that our emailing service stopped working or wasn’t running, it just didn’t send email to Kofi Mensah but probably sent an email to another user after.This is where distributed tracing comes in. As a request propagates between multiple services it leaves a trace which can be used to track the life cycle of the request.Whats a trace?A trace is just a record of how a request is propagated between services, the life cycle of a request. This is normally a collection of spans depending on the journey of the request.A span is the smallest unit in distributed tracing, it normally consists of an id,parent id to indicate if span is connected to another and tags to give more information on the span and other meta data.There are so many tools for recording and visualizing traces , eg Jaeger, OpenTracing, Opencensus etc.. but this post is focused on two of them, Opencensus and Spring Cloud Sleuth and how to visualize trace data in Kibana and Zipkin.Zipkin is a distributed tracing system. It helps gather timing data needed to troubleshoot latency problems in microservice architectures. It manages both the collection and lookup of this data. Zipkin’s design is based on the Google Dapper paperAdded to these, Zipkin also has a Rest API, two version of it v1 and v2. This makes Zipkin a polyglot server for visualizing traces and effectively see the intercommunication of your services. Hard to find a language that doesn’t support REST.But what if we want more, for example visualize more data from traces and understand little more of our services in production that we can’t get from logs. This is where the ELK stack comes in. The ELK stack is really awesome and if you don’t know about it you can read more on it with a Google search.Since the goal of this post is to find ways to understand traces for your system, I’ll show how to use two distributed tracing libraries to send trace data to Zipkin and the ELK stack.The sample applications would help us understand what each library,that is Sleuth and Opencensus, give us and how sending the trace to Zipkin and/or ELK can help us understand our distributed systems in production and the value each framework brings to the overall goal.2. Distributed Tracing toolsa. Using Spring Cloud SleuthFor Java/Kotlin projects using the Spring stack it should be quite easy to get started with this. Since this requires almost nothing to get it started with. You can check out the sample project from github.My reason for picking Spring Cloud Sleuth is the simplicity. It requires almost nothing to set it up and once you have thedependency as part of your project it automatically does the tracing for you.The dependency which you can find below:&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;&lt;/dependency&gt;b. Using OpencensusThis an opensource distributed tracing tool started Google.The reason I picked this was in as much as the “magic” that Spring Cloud Sleuth is, there might be certain uses magic won’t work and also one that is not dependent on the Spring stack and not only limited to the JVM. This is where Opencensus comes in.4. Why an Elasticsearch Trace ExporterI love the ELK stack. I’ve used all or parts of it for operational analysis, data analytics,data lake,data warehouse, data prediction in different projects. I’ve actually written some FOSS projects for Elasticsearch as well.One of the ways I love to work on FOSS projects is that at the end of it all, I want anyone to be able to use a combination of my opensource projects. So I try to find ways to link all the FOSS projects I work on.I picked Elasticsearch based on my past experience with it, Kibana gives us so much power to visualize the data in Elasticsearch and some of the FOSS projects I’ve worked on which when put together can enable someone generate trace data reports pdf,csv or html, configure alerts based on certain metrics via sms,email and api and also build an ML algorithm on trace data to predict performance of your services.From the FOSS links you can tell some of them are still in progress but you get the idea connecting them.The best reason of all is search. Being able to search traces to know why a service request took 20 secs on Monday 21st September at 3:43 PM for Kofi accessing the service from Tokyo rather than the 5 seconds we have as SLA for response time. This possible with Elasticsearch.a. Spring Cloud Sleuth Elasticsearch Exporter *Spring Cloud Sleuth doesn’t support exporting to Elasticsearch. So this might be a little confusing. To send trace data from a Spring Boot project to the ELK stack would require Logstash. The “L” in the ELK.Logstash is an open source, server-side data processing pipeline that ingests data from a multitude of sources simultaneously, transforms it, and then sends it to your favorite “stash.”In our case the stash is Elasticsearch. You can find the logstash config we used for our sample project here. A simple config that sends logs in a particular format to logstash running on 127.0.0.1 port 5000 from the config file.b. Opencensus Elasticsearch ExporterOpencensus doesn’t have an Elasticsearch exporter either. I’ve worked on and submitted a PR to the Opencensus team. Once it gets accepted and released you can find more details on it under the exporters in Opencensus.For now you can check out on my personal github for the Elasticsearch exporter I wrote for Opencensus.5. Why an Zipkin Trace ExporterSpring Cloud has a simple way of starting a Zipkin server. This is actually based on OpenZipkin which originated from Twitter.Using it is as simple as using the annotation @EnableZipkinServer on our main class. From the Spring Cloud Zipkin documentation:The Zipkin Server delegates writes to the persistence tier via a SpanStore. Presently, there is support for using MySQL or an in-memory SpanStore out-of-the-box. As an alternative to REST, we can also publish messages to the Zipkin server over a Spring Cloud Stream binder like RabbitMQ or Apache Kafka. We’ll use this option, and org.springframework.cloud:spring-cloud-sleuth-zipkin-stream’s @EnableZipkinStreamServer, to adapt incoming Spring Cloud Stream-based Sleuth Spans into Zipkin’s Spans and then persist them using the SpanStore.This is really cool especially if you’re the type that’s really strict on “REST intercommunication between internal services is bad” or just prefer TCP to HTTP for recording traces.a. Spring Cloud Sleuth Zipkin ExporterTo get our sample Spring Cloud Sleuth to send trace data to Zipkin is just to add this line in our properties file1.Where our zipkin server is locatedspring.zipkin.baseUrl=http://localhost:94112.To enable this functionality in our app and also make sure we have this dependency&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt;&lt;/dependency&gt;There’s also another configuration to check the percentage of data we send to zipkin. The default is 10%. If you want 50% just add this to your properties file. Note that although it is a percentage, the accepted value is a double between 0.1 to 1.0 where 0.1 is 10%.spring.sleuth.sampler.probability=0.5If you’re already on Spring Boot 2.0 you can also use annotations to create our own spans. You can find an example of this usage on github and also read more on it here. This requires these dependencies or later:&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt; &lt;version&gt;2.0.0.M9&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt; &lt;version&gt;2.0.0.M9&lt;/version&gt;&lt;/dependency&gt;b. Opencensus Zipkin ExporterThe team at Opencensus already wrote a zipkin exporter with an example on how to export trace data with it.You can find the documentation and codes here.Hopefully with this post I hope you can appreciate what each library gives us and the part Zipkin and ELK stack play in visualizing traces in a distributed system and can help you make a decision with the attached sample project. Codes are available on githubREFERENCEShttp://microservices.io/patterns/observability/distributed-tracing.htmlhttps://cloud.spring.io/spring-cloud-static/spring-cloud-sleuth/2.0.0.M9/single/spring-cloud-sleuth.html" }, { "title": "Elasticsearch Report Engine", "url": "/posts/Elasticsearch-Report-Engine/", "categories": "oss", "tags": "oss, elasticsearch", "date": "2017-06-17 00:00:00 +0200", "snippet": "Plugin to generate Reports from Elasticsearch Queries. Basic Overview Install Usage PDF HTML CSV Download Scheduling Contribute Code of Conduct LicenseOverviewOnce this plugin is installed into elasticsearch search,it exposes the url http://{ip}:9200/_generate, you can run queries on your cluster with the right parameters it would return PDF,HTML or CSV file.Install1. Install pluginsudo bin/elasticsearch-plugin install https://github.com/malike/elasticsearch-report-engine/releases/download/5.4.0/st.malike.elasticsearch.report.engine-5.4.0.zip2. Grant permissions3. Folder StructureCreate folders templates and reports in ES_HOME. Store your *.jasper and *.jrxmlfiles in the templates folder and pass the templateName as the template (with the right extension) parameter for HTML and PDF reports.UsagePDF1. PDF ReportThe plugin uses Jasper Report as core engine for generating PDF reports.PDF templates can be designed using iReport Designer. Thisgenerates a jrmxl file. You can also use the compiled file with the extension jasper.The plugin generates base64 encoded stream of the PDF report generated onceyou pass the name of the template file and the query to fetch data from Elasticsearch.PDF Sample Request curl -H \"Content-Type:application/json\" -XPOST \"http://localhost:9201/_generate\" -d '{\"format\":\"PDF\",\"fileName\":\"TEST_REPORT\",\"index\":\"reportindex\",\"template\":\"filename.jrxml\",\"from\":0,\"size\":10,\"query\":\"{term:{description:Transaction}}\"}'Parameters i. format : Format of Report [Required] ii. index : Elasticsearch Index [Required] iii. template : Jasper Report Template [Required] iv. from : Offset for querying large data [Optional] v. size : Size for querying large data [Optional] iv. query : Query to search Elasticsearch index [Optional : Defaults to ‘’ if nothing is passed]** vi. *fileName : File name **[Optional] Generate PDF Responsei. Success {\"status\":true, \"count\":1, \"data\": \"base 64 encoded string\", \"message\":\"SUCCESS\" }ii. Missing Required Param {\"status\":false, \"count\":0, \"data\": null, \"message\":\"MISSING_PARAM\" }iii. Report Format Unknown {\"status\":false, \"count\":0, \"data\": null, \"message\":\"REPORT_FORMAT_UNKNOWN\" }iii. System Error Generating Report {\"status\":false, \"count\":0, \"data\": null, \"message\":\"ERROR_GENERATING_REPORT\" }Sample PDFHTML2. HTML ReportJust like the PDF report,the HTML also uses Jasper Report as core engine for generating reports.HTML Reports provides an alternative for use cases where reports should not be sent as an attached file.The generates base64 encoded stream of the HTML report generated.There’s also an option to return the HTML string instead of the base64 encoded string. This requires passing returnAs:PLAIN as part of the request JSON.HTML Sample Request curl -H \"Content-Type:application/json\" -XPOST \"http://localhost:9201/_generate\" -d '{\"format\":\"HTML\",\"fileName\":\"TEST_REPORT\",\"index\":\"reportindex\",\"template\":\"filename.jrxml\",\"from\":0,\"size\":10,\"query\":\"{term:{description:Transaction}}\"}'Parameters i. format : Format of Report [Required] ii. index : Elasticsearch Index [Required] iii. template : Jasper Report Template [Required] iv. from : Offset for querying large data [Optional] v. size : Size for querying large data [Optional] iv. query : Query to search Elasticsearch index [Optional : Defaults to ‘’ if nothing is passed]** vi. *fileName : File name **[Optional] vii. returnAs : How you want HTML file returned. Possible values PLAIN and BASE64 [Optional : Defaults to BASE64] Generate HTML Responsei. Success {\"status\":true, \"count\":1, \"data\": \"base 64 encoded string\", \"message\":\"SUCCESS\" }ii. Missing Required Param {\"status\":false, \"count\":0, \"data\": null, \"message\":\"MISSING_PARAM\" }iii. Report Format Unknown {\"status\":false, \"count\":0, \"data\": null, \"message\":\"REPORT_FORMAT_UNKNOWN\" }iii. System Error Generating Report {\"status\":false, \"count\":0, \"data\": null, \"message\":\"ERROR_GENERATING_REPORT\" }Sample HTML*Note: For HTML reports you want returned as HTML string instead of a base64 encoded string.Send this parameter as part of your default parameters : “returnAs”:”PLAINCSV3. CSV ReportUnlike the PDF and HTML reports,the CSV option does not use Jasper Report as core engine for generating reports.Generating a CSV report uses the query and returns a base64 encoded of the file.CSV Sample Request curl -H \"Content-Type:application/json\" -XPOST \"http://localhost:9201/_generate\" -d '{\"format\":\"CSV\",\"fileName\":\"TEST_REPORT\",\"index\":\"reportindex\",\"from\":0,\"size\":10,\"query\":\"{term:{description:Transaction}}\"}'Parameters i. format : Format of Report [Required] ii. index : Elasticsearch Index [Required] iii. returnAs : How you want CSV file returned. Possible values PLAIN and BASE64 [Optional : Defaults to BASE64] iv. from : Offset for querying large data [Optional] v. size : Size for querying large data [Optional] iv. query : Query to search Elasticsearch index [Optional : Defaults to ‘’ if nothing is passed]** vi. *fileName : File name **[Optional] CSV Sample Responsei. Success {\"status\":true, \"count\":1, \"data\": \"base 64 encoded string\", \"message\":\"SUCCESS\" }ii. Missing Required Param {\"status\":false, \"count\":0, \"data\": null, \"message\":\"MISSING_PARAM\" }iii. Report Format Unknown {\"status\":false, \"count\":0, \"data\": null, \"message\":\"REPORT_FORMAT_UNKNOWN\" }iii. System Error Generating Report {\"status\":false, \"count\":0, \"data\": null, \"message\":\"ERROR_GENERATING_REPORT\" }Sample CSV*Note: For CSV reports you want returned as comma separated values instead of a base64 encoded string.Send this parameter as part of your default parameters : “returnAs”:”PLAINDownloadElasticsearch versions supported by this plugin include : Elasticsearch Version Report Plugin Version Comments 5.4.0 zip Tested 5.6.10 zip Tested SchedulingThis plugin can work with an alerting system and a custom elasticsearch watcher to send emailed reports to specific contacts. By creating your watcher events in the custom elasticsearch watch, events would be pushed to Apache Kafka once there’s a hit, go-kafka-alert listening on Apache Kafka for events would react by emailing embedded HTML reports or attached CSV or PDF reports.This requires no updates to this plugin but setup and configurations in go-kafka-alert and elasticsearch watcherContributeContributions are always welcome!Please read the contribution guidelines first.Code of ConductPlease read this.LicenseGNU General Public License v3.0" }, { "title": "Why TDD and How To With Fluentlenium, RestAssured, HamCrest and Mockito", "url": "/posts/Why-TDD-and-How-To/", "categories": "tech", "tags": "tdd, test, test-driven-development", "date": "2017-05-07 00:00:00 +0200", "snippet": "1. What’s TDD?It stands for Test Driven Development. Simply meaning the Test Drives the Development(Coding). This is done in small parts and starts with writing a test for a feature.This involves a cycle of this 3 steps in small parts.a. Red Bar (Fail): Code for specification that fails.b. Green Bar(Pass) : Code and test enough to make sure specification passes.c. Refactor, Retest (Cleanup)Note that the order matters. If step 2 comes before step 1. It’s not TDD.### 2. Why you should \ta. The next dev : “if small is good, then smaller must be better” - Bob MartinSometimes we tend to get locked in and code the specs away with just the end user in mind. One very important person we forget is the next dev. The next developer or engineer who is going to modify code.Unit Testing in TDD forces devs to break spaghetti code into smaller units. The fact is every good developer should know how tobreak any service into units and not write codes over a certain number of lines.Its not something that comes just with TDD. Sticking to TDD’s guidelines enforces you to break spaghetti into smaller units. \tb. Faster test cyclesI wasn’t always on the TDD train. It felt rather long and unit testing (a central part of TDD ) doesn’t really reflect how the end user interacts with the product. So I placed high priorities on end-to-end tests and integration tests using this cycle:a. Code for productionb. Test your end points or APIs (Integration test).c. Perform an end to end after putting the whole application together.The problem with this flow is I relied a lot on integration and end to end tests. Which are generally slow.For example an end to end test of an end user updating their profile page would be blocked if the login end to end test fails.Testers would have to wait till the login works perfectly before moving on to test the profile update. Some of the bugs that only show up during end to end tests could have being discovered during unit test.In summary, unit tests help to discover bugs just faster than integration tests, they also help to isolate bugs and are far more reliable to help devs discover bugs before end to end tests.c. Discover Regression Errors Faster“A software regression is a software bug which makes a feature stop functioning as intended after a certain event (for example, a system upgrade, system patching or a change to daylight saving time)”Once you start refactoring your code for a feature, another service that depends on your newly refactored service would either fail or pass.TDD would help you reduce regression errors. Once a test that used to pass starts to fail you know you have to fix before pushing to production. \td. ConfidenceOverall confidence your deliverable increases. The specification doesn’t just work on your machine but works Production,UAT and any other dev’s machine.Well, there will still be bugs but not as much as with TDD.3. A Sample TDD ProjectUsing Spring Boot,let’s build a sample project using TDD principles. The project would be a simple application to help users signup.The dependencies needed for testing are listed below. &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.fluentlenium&lt;/groupId&gt; &lt;artifactId&gt;fluentlenium-core&lt;/artifactId&gt; &lt;version&gt;0.10.9&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.fluentlenium&lt;/groupId&gt; &lt;artifactId&gt;fluentlenium-assertj&lt;/artifactId&gt; &lt;version&gt;0.10.9&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.jayway.restassured&lt;/groupId&gt; &lt;artifactId&gt;spring-mock-mvc&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;type&gt;jar&lt;/type&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.hamcrest&lt;/groupId&gt; &lt;artifactId&gt;hamcrest-core&lt;/artifactId&gt; &lt;version&gt;1.3&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mockito&lt;/groupId&gt; &lt;artifactId&gt;mockito-core&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;What the dependencies give us?1. Fluentlenium.“FluentLenium helps you writing readable, reusable, reliable and resilient UI functional tests for the browser.”2. RestAssured and HamCrest give us the opportunity to test rest service and validate.3. Mockito“Mockito is used to mock interfaces so that a dummy functionality can be added to a mock interface that can be used in unit testing. This tutorial should help you learn how to create unit tests with Mockito as well as how to use its APIs in a simple and intuitive way…“Let’s startWe can break down the project in 2 parts. i. Frontend ii. HTTP API to process the request from the frontend. Create a spring boot project with thymeleaf and h2 dependencies. Write the first functional test for the signup page. \t@RunWith(SpringJUnit4ClassRunner.class)\t@SpringApplicationConfiguration(classes = AppMain.class)\t@WebAppConfiguration\t@IntegrationTest(value = \"server.port:9000\")\tpublic class SignupControllerUITest extends FluentTest{\t\tpublic WebDriver webDriver = new FirefoxDriver();The webDriver is what Fluentlenium uses to browse natively as a user would. The FireFoxDriver options works fine because it comes as default. Our current setup requires FireFox 38.I won’t advice using the ChromeDriver unless you can ship the driver based on OS with your code and switch based on the other developers you work with. AFAIK It requires you set a path to the ChromeDriver which doesn’t make the code developer friendly.Another option is the HtmlUnitDriver. However it won’t open the browser natively but it would also test the page. Lets create our first test : Can I see the signup page?\t@Test\tpublic void testViewSignupPage() throws Exception {\t goTo(\"http://localhost:9000/signup\");\t FluentLeniumAssertions.assertThat(find(\".header\")).hasText(\"Signup\");\t}This would obviously fail because we don’t have the signup page. So we fix it by creating our signup page. So our test would pass.To fix this, we’ll create a controller for our signup page @RequestMapping(value = \"/signup\",method = RequestMethod.GET)\tpublic String signup() {\t return \"signup\";\t}Test, let it fail then create the view for our signup page in “/resources/template”. This is based on the default config for thymeleafWe’ll make sure we have an element the css selector “.header” and has text “Signup” to pass our test..Next we would like to create a form for our signup page. So we create a test that checks our signup page for the form.This would fail. We would move on to create the signup page to pass that test.Once passed,we’ll create a test that fills our signup form and submits.\tAssert.assertEquals(\"SUCCESS: \"+USERNAME+\" saved.\",webDriver.switchTo().alert().getText());This would give us another failure because we are yet to code our signup page to show any alert although our test expects an alert.After adding the jquery code to submit the form details to our HTTP API_ with the help of an alert we can know whether it works or not.Note that Fluentlenium offers lots of locators you can use to validate as well.After refactoring our code in the view to show a success alert or failure alert we are one step close since we don’t get this exceptionorg.openqa.selenium.NoAlertPresentException: No alert is present (WARNING: The server did not provide any stacktrace information)but thisorg.junit.ComparisonFailure:Expected :SUCCESS : malike saved.Actual :ERROR:405We refactor to fix this as well. We fix this red by creating the api for saving user data. We create a test for our signup HTTP API in theSignupControllerTest to first return 200.Then another test to confirm your details get saved once the parameters are passed to the api. This failing test would require us to createa UserServiceTest class.We continue with this cycle of creating a test,which would fail,refactor till its passes. Then we move on to the next specification.Check out the source code. The test cases for the the signup page can be found hereand the test cases for the api here and the tests for our UserServiceTest here.The Frontend test uses Fluentlenium to help us use the browser natively as our end user would. The Controller test uses RestAssured to test our HTTP endpoint and HamCrest to help us validate the response.The UserService test uses Mockito to help us mock UserService and all the services it depends on,which in our case is just UserRepository. You can see that although the Frontend,the SignUpController and the UserService tests are different they follow the same TDD guidelines. Breaking into bits and writing the tests before the feature and refactoring till the pass.Although a bit limited in features,with this example we’ve been able to create test cases to test our frontend using Fluentlenium and also using Mockito,RestAssured and HamCrest test our service and HTTP API.After 12 test cases,all green, we’ve built a signup feature using TDD.. Our sample project can be found hereREFERENCEShttp://www.jamesshore.com/Agile-Book/test_driven_development.htmlhttps://testing.googleblog.com/2015/04/just-say-no-to-more-end-to-end-tests.html" }, { "title": "Creating An In-Memory FIFO Cache", "url": "/posts/Queue-LinkedHashMap/", "categories": "tech", "tags": "documentation, sample", "date": "2017-03-12 00:00:00 +0100", "snippet": "A well implemented HashMap in Java would give you performance of O(1) for get and put.Well implemented in this case means your hashCode and equals are helpful to reduce collisions.From Java 7 to Java 8 the HashMap implementation changed from using linkedList to balance trees to resolve collisions, this means for very bad implementation (sometimes there are just collisions) of hashCode which may result in many collisions we’ll no longer have a LinkedList of items,which would give us O(n) worst case. Balance trees improve this with a O(log n) from Java 8.This article talks about it in depth.Now to the main point of this article, there are many In-Memory Key-Pair databases there. Eg: Redis. But just in case you want one built with HashMap data type. Which can get you performance of O(1) for both put and get without using a framework HashMaps would be our go-to data structure.But the problem with using HashMap as a cache is,if you don’t control the size it might blow up your memory.The default HashMap can grow in size.If memory is not your problem you are good to go with HashMap but if you want to control the size and the items to be stored with FIFO priorities then let’s continue.Unfortunately HashMaps don’t maintain the order of items when inserted. Why do we need the order?, we need the order to determine what gets to leave the queue. That is know the order the items were entered.LinkedHashMap however maintains the order and also has removeEldestEntryBy simply extending the properties of removeEldestEntry. We can build an in-memory cache with fixed size HashMap with O(1) performance for get and push operations.\tpublic class HashMapCache&lt;K,V&gt; extends LinkedHashMap{\t\tprivate final int maxSize;\t public FixedSizeHashMap(int maxSize) {\t this.maxSize = maxSize;\t }\t @Override\t protected boolean removeEldestEntry(Map.Entry&lt;K, V&gt; eldest) {\t return size() &gt; maxSize;\t }\t @Override\t public boolean equals(Object obj){\t return obj==this;\t }\t @Override\t public int hashCode(){\t return System.identityHashCode(this);\t }\t}Lets write a test to test our implementation. You can see I’ve not written tests for all the behaviors of a ListHashMap.Our main focus is to see if your canPushEarliestOut() would be green.\tpublic class HashMapCacheTest{\t@Before\t public void setup() {\t fixedSizeHashMap.put(\"one\", \"one\");\t fixedSizeHashMap.put(\"two\", \"two\");\t fixedSizeHashMap.put(\"three\", \"three\");\t }\t @Test\t public void hasRightSize() {\t Assert.assertEquals(3, fixedSizeHashMap.size());\t fixedSizeHashMap = new FixedSizeHashMap&lt;&gt;(5);\t Assert.assertEquals(0, fixedSizeHashMap.size());\t }\t @Test\t public void canAdd() {\t Assert.assertTrue(fixedSizeHashMap.containsKey(\"one\"));\t Assert.assertEquals(3, fixedSizeHashMap.size());\t }\t @Test\t public void canGet() {\t Assert.assertTrue(fixedSizeHashMap.containsKey(\"one\"));\t Assert.assertTrue(fixedSizeHashMap.containsKey(\"two\"));\t Assert.assertTrue(fixedSizeHashMap.containsKey(\"three\"));\t Assert.assertEquals(3, fixedSizeHashMap.size());\t }\t @Test\t public void canPushEarliestOut() {\t Assert.assertTrue(fixedSizeHashMap.containsKey(\"one\"));\t Assert.assertEquals(3, fixedSizeHashMap.size());\t fixedSizeHashMap.put(\"four\", \"four\");\t Assert.assertFalse(fixedSizeHashMap.containsKey(\"one\"));\t Assert.assertTrue(fixedSizeHashMap.containsKey(\"four\"));\t Assert.assertEquals(3, fixedSizeHashMap.size());\t }\t}REFERENCEShttps://dzone.com/articles/hashmap-performance - Tomasz Nurkiewicz" }, { "title": "Exploring LDAP Integration With Spring's AuthenticationProvider,OAuth2 and MongoDB for a SSO service", "url": "/posts/Spring-Security-OAuth2-With-LDAP/", "categories": "distributed", "tags": "documentation, sample", "date": "2016-06-23 00:00:00 +0200", "snippet": "In this post I talked about using Spring Security OAuth2 and MongoDB (or any database of your choice).Today we are going explore the AuthenticationProvider in spring by building LDAP or Active Directory authentication into our SSO microservice which can be used by clients or users.What is LDAP? I’m guessing you already know what it is thats why you got here.Spring Security supports LDAP authentication out of the box. As the title suggests weare building a custom one using Authentication Provider interface.This explains how Spring Authentication Provider interface works read this.For this project I forked the codes from Spring Security OAuth2 with MongoDB.Adding our LDAP Authentication Provider would require1. LDAP dependencies.\t&lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-ldap&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.ldap&lt;/groupId&gt; &lt;artifactId&gt;spring-ldap-core&lt;/artifactId&gt; &lt;version&gt;2.0.4.RELEASE&lt;/version&gt; &lt;/dependency&gt;2. Implementing Auth Provider Interface.Spring Security AuthenticationProvider can be implemented to create a custom auth provider.The most important part of AuthenticationProvider is authenticate. Which “Performs authentication with the same contract as AuthenticationManager.authenticate(Authentication)”In our custom implementation of authenticate we would use configured LDAPTemplate to validate user’s credentials and once authentication is a success we can do anything we want. In my case I check if a user exists before I persist their details.3. Configure.To configure our custom AuthenticationProvider we just need to let Spring Integration know our custom provider. By default it would pass all authentication details to all configured AuthenticationProviders for either a successful response or an exception.With this configuration LDAP users would get OAuth2 tokens just as the users we created in Spring Security OAuth2 with MongoDB. So what we’ve built here is a single sign on authentication microservicethat works for users in MongoDB and LDAP. In due time we would add social and other custom authentication providers.4. Changes To NoteLDAP Bean\t&lt;bean id=\"userLDAPAuthenticationProvider\" class=\"st.malike.auth.server.service.security.UserLDAPAuthProviderService\" /&gt;New Authentication Provider\t &lt;sec:authentication-manager alias=\"authenticationManager\" &gt;\t \t &lt;!--new authentication provider--&gt;\t &lt;sec:authentication-provider ref=\"userLDAPAuthenticationProvider\" /&gt;\t &lt;sec:authentication-provider ref=\"userAuthProviderService\" /&gt;\t &lt;sec:authentication-provider\t user-service-ref=\"clientDetailsUserService\"&gt;\t &lt;sec:password-encoder ref=\"passwordEncoder\" /&gt;\t &lt;/sec:authentication-provider&gt;\t &lt;/sec:authentication-manager&gt;The actual code doing the validation is here which can you can update based on how the integration with your single sign on microservice works with the LDAP serverFinally check this LDAP injection With AuthenticationProvider we can build custom authentication providers to validate from multiple services once we can ‘hook’ it in the AuthenticationManager.And keep that in mind not to make the overall authentication expensive because every authentication request would go through all the authentication providers.FYI source codes on Github." }, { "title": "Design Thinking Approach For Hobby Robotics Project - Part 2", "url": "/posts/Design-Thinking-Approach-For-Robotic-Hobbyist-2/", "categories": "electronics", "tags": "documentation, sample", "date": "2016-04-05 00:00:00 +0200", "snippet": "Ok, this is the long overdue part two of where we left off. What are we doing today? 1. Give it a name2. Components from prototype3. Program4. Simulation1. Give it a nameBecause its movement is modeled after a bug we (I) should call it Bug1.2. Check prototype and pick components.If this is not your first electronic project you probably would have identified some components in the prototype. Like the Ultrasonic Sensor and the servos. Two servos one for the front and rear legs formovement.New to the hobby servo? What about the HC-SR04 ultrasonic sensor?i. Front Legs with a ServoBug1 uses two hobby servos for movement. With the arduino as the command. But the position of the two servosto give the required movement is not that straight forward.Honestly this was what I wanted but the point of this post is to focus on the methodology not the actual project. I had to create a constant reminder of this because I kept drifting away.From the diagram above, if the part labelled 2 (front right leg) moves forward that is 120 degrees 3 (front left leg)would be 60 degrees. The back and forth of this movement would push Bug1 forward.This servo can be controlled to steer Bug1 as well. The front servo is mainly responsible for how Bug1 moves and where it turns. Specifications&nbsp;\tOperating Speed: 0.17sec / 60 degrees (4.8V no load)Operating Speed: 0.13sec / 60 degrees (6.0V no load)Stall torque:12kg/cm(6V)Operation Voltage : 4.8 - 7.2 V Temperature range: 0°C to 55°CPower Supply: Through External AdapterLength 300mm&nbsp;ii. Rear Legs with a ServoThe servo controlling the rear legs is responsible for supporting Bug1. This enables it to have good composurewhen moving or not. Think of it as the second pair of legs designed to make Bug1 stand like a table. Its placed such that its movement is restricted to just an up and down movement.iii. Eyes with the HC-SR04 UltraSonic SensorThe UltraSonic sensor helps Bug1 see and sense the environment to help in the way it reacts to the environment. Specifications&nbsp; Power Supply :+5V DC Working Current: 15mA Ranging Distance : 2cm – 400 cm/1″ – 13ftMeasuring Angle: 30 degreeTrigger Input Pulse width: 10uSDimension: 45mm x 20mm x 15mm&nbsp;It works simply by sending an ultrasound of about 40Khz and then listens for the pulse to echo back in time,t.We can calculate the distance of the object from the sensor (Bug1) as Distance from object (m) = 340 (m/s) * t(s) / 2Because the speed of light is approx 340 m/s at room temperatureiv. Bluetooth ReceiverAlthough not shown in our prototype this is the connection between our command center and the Bug1. We can send and recieve data from Bug1. Specifications&nbsp;Extremely small radio - 0.15x0.6x1.9\"Very robust link both in integrity and transmission distance (18m)Hardy frequency hopping scheme - operates in harsh RF environments like WiFi, 802.11g, and ZigbeeEncrypted connectionFrequency: 2.402~2.480 GHzOperating Voltage: 3.3V-6VSerial communications: 2400-115200bpsOperating Temperature: -40 ~ +70CBuilt-in antennaDimensions: 45mm x 16.6mm x 3.9mm&nbsp;v. “Sticky” LegsWell Bug1 has to move right?. With the help of metal bended legs we can make this happen.vi. Arduino UnoBug1 needs to process.vi. BatteriesSince we using an Arduino any 5V battery would do.3. ProgramBug1 is programmed using C.The codes and other files for the project can be found here. But note that changes can be made to when we start building the project.4. Simulation.Before we jump in soldering and building Bug1 lets first run a simulation to see how our code would run. Fortunately for us there are tons of tools to run this simulation which you can find here. I chose Protues.If you decide to also use Protues check this or just use the codes with any simulation tool.We can safely test our codes and wiring with ease.If you’ve ever used Protues you’d know that some components do not come out of the box. But you can get them online.I’m using this Arduino - Protues and HC-SR04 Ultrasonic Sensor - Protues. Next up we get to build Bug1. Hopefully the components I ordered would arrive by then Simulate the serial communication and automated movement of Bug1 Check out proejct files on GitHub." }, { "title": "... a little more with spring metrics", "url": "/posts/Little-More-With-Spring-Metrics/", "categories": "tech", "tags": "documentation, sample", "date": "2016-02-17 00:00:00 +0100", "snippet": "If you read this, we talked about capturing and reading application metrics.In this post I’ll show you how to do a little more with that data using Spring Metrics and Lambda Architecture which we also talked about.I’ll categorize this post under two sections capturing time series of metrics and persisting metrics1. Capture Time SeriesThere are two ways to get this done. One of them is hack for the purpose of this post, because it makes no use of Spring Metrics or the Lambda Architecture.I’m assuming we know how the Lambda Architecture works. The only addition to make this work for our purpose is we feeding the Lambda Architecture stream with metric events and then leaving it for our Lambda Architecture. Everything else remains the same as with this approach of capturing the metric. The results would be an aggregated time series of the results persisted in our batch layer with the speed layer giving us real time or near real time events of the data.Now the hack, this solely relies on the memory by keeping all aggregated events stored in memory.See how-to below.Declare your variables private LinkedHashMap&lt;String, LinkedHashMap&lt;String, LinkedHashMap&lt;String, Integer&gt;&gt;&gt; timeMap = new LinkedHashMap&lt;&gt;(); private static SimpleDateFormat dateFormat = new SimpleDateFormat(\"dd-MM-yyyy HH:mm\");Increase metricpublic void increment(String metric) { String time = dateFormat.format(new Date()); LinkedHashMap&lt;String, LinkedHashMap&lt;String, Integer&gt;&gt; hashMap = timeMap.get(metric); hashMap = (timeMap.isEmpty() || null == hashMap) ? new LinkedHashMap&lt;&gt;() : hashMap; LinkedHashMap&lt;String, Integer&gt; statusMap = hashMap.get(time); statusMap = (null == statusMap) ? new LinkedHashMap&lt;&gt;() : statusMap; Integer count = statusMap.get(metric); count = (null == count) ? 1 : count++; statusMap.put(metric, count); hashMap.put(time, statusMap); timeMap.put(metric, hashMap);}As you can see it’s just for increasing a metric, for decreasing a metric I’m sure you can figure that part out.2. Persisting to databaseIf you decide to Lambda you can bridge the channel reading the stream and the channel writing to Apache Kafka.If you decide to not Lambda then build a simple schedule job that writes the metricdata to the database. You can easily get this done with something like this.@Configurationpublic class MetricDBConfig { @Scheduled(initialDelay = 60000, fixedDelay = 60000) void saveMetric() { //Save to DB for singular or all metrics } public LinkedHashMap getSingularMetric(String metric) { //use this to get time series for a metric return timeMap.get(metric); } // public LinkedHashMap getAllMetric() { return timeMap; //all of 'em }}For the Lambda Architecture picking the event metrics to be written to Apache Kafka would be slightly different.@Configurationpublic class MetricDBConfig { @Autowired private MetricRepository repository; @Scheduled(initialDelay = 60000, fixedDelay = 60000) void saveMetric() { //Save to Apache Kafka }} Ok." }, { "title": "Customer Stickiness For Your Enterprise", "url": "/posts/Customer-Stickiness-For-Your-Enterprise/", "categories": "stuff", "tags": "documentation, sample", "date": "2016-01-23 00:00:00 +0100", "snippet": "What is Customer Stickiness?“The degree to which the existing use of a product or service encourages its continued use as opposed to that of a competitor”.- investorwords.comA little about Enterprise Nurs,a DreamOvalproduct which is an innovation framework that helps enterprises deliver innovative customer-centric services essential to the enterprise in a quicker way.Now that I’ve defined the two, let me tell you how to drive customer stickiness with Enterprise Nurs with these five simple steps.1. Learn about your customerKnowing your customer means, they mean a lot more to you than the transactions they have with your organization. Helping your organization continue the conversation after the transaction which you can only do well if you know your customer.Enterprise Nurs has KYC widgets - Core and Customized. Core widgets come out of the box with Enterprise Nurs. Customized widgets can be integrated to show varying KYC data which is required by the enterprise to learn more about their customers.2. Customer Complaints.Customers complain. They’ll always do, but it is not always bad. Customer complaints are like second chances, grab it and redeem yourself. Most organizations have support staff but segregated from the administration of their core system.Enterprise Nurs’ ESB enables organizations to integrate their support systems. This enables the organizations to continue the conversation started by support staff no matter how segregated the two systems are. Support can tell what services or actions customers have initiated or in the process of completing before calling support and maybe contact customers before they call. This type of preempting from an organization tells its customers that they care.Enterprise Nurs gives organizations another opportunity to redeem themselves through this omni-channel.3. Extensive monitoring of Customer’s Journey.Once a customer signs up with your enterprise they are on a journey. A journey from acquisition to retention. With rich availability of metrics the customer’s journey can be monitored and evaluated. These metrics provide rich visibility into the the customer’s retention pipeline, this data can be used by marketers to pin point when and where customers can be given more value.Enterprise Nurs tackles this problem with these two services - Customer Stream and LNS. Customer Stream is a real time activity stream for customers. Enterprise Nurs’ LNS (Live Notification Service) feeds of this stream to send notifications via any channel to the customer or any stakeholder in the customer’s journey with the organization.The usage of Enterprise Nurs helps organizations not to be reactive. An enterprise will then be able create more value for its customers, value good enough to retain them.4. BI ReportsBI tools help in making decisions. Regular analysis from BI tools gives up-to-date information organizations would need. organizations are able to see trends, notice changes and know when and where to improve the customer retention pipeline to meet the organization’s target.Enterprise Nurs provides a self-service report engine where enterprises can do this with no stress. Enterprise Nurs’ ESB unifies data from multiple sources to provide a rich interactive report that can be represented as widgets in the application front-end, emailed regularly as a PDF or XLS, and oh, with drill-downs. Drill-down data that go steps deeper to show what data points make up the rich interactive display.5. Adoption..and YES steps one to four are not rocket science. They can’t be filed under a discovery of the day. They’ve been mentioned over and over again. They’ve been part of your organization’s annual or quarterly goals for years now.Most problems associated with creating sticky customers evolves around the technology solutions, processes used and adoption of the two by staff and this is where Enterprise Nurs comes in For more contact the Enterprise Nurs team" }, { "title": "Happy New Year", "url": "/posts/Happy-New-Year/", "categories": "stuff", "tags": "documentation, sample", "date": "2016-01-02 00:00:00 +0100", "snippet": "2015 was kind of great. But also a disappointing one that once again confirmed the Babel Theory.What is the Babel Theory?I’m guessing you’ve ever heard the tower of Babel story. If not read this.Currently we are not able to build our Tower of Babel not because we speak different languages. Duh we haveGoogle Translate for that. But the app store. Everyone is focused on putting something on the AppStore,PlayStore,Windows shop or some other store. Funny thing is most of them can all be categorized under Uber for this or Messaging for that.With this huge number doing this everyday no wonder we call a two-wheel scooter a hover board.All the same it was a good year. I mean lets take SpaceX’s Falcon9,Blue Origin’s New Shepard,Tesla’s PowerWall. Bill and Melinda Gates work with their Foundation, Elon Musk and companies. These are just few of the solutions and people who inspired me in 2015.2015 is gone. There were lots of things I wanted to do last yearthat couldn’t. I read a blog about how an indie game dev developed 12 games in a year.One every month. I wanted to do the same in 2015. It was terrible. Very terrible.Well,I’ll try again.If you followed SpaceX and Blue Origin rocket launch to space for 2015, you’ll notice lots differences. And what really caught my attention has nothing to do with the type rockets or limitations to suborbital flights. These are obvious or lets say out there. The public relation is what caught my attention. SpaceX publicized their launch so the world saw their 2 failed attempts,realtime, before the success. But for Blue Origin we saw only their success. Was that their first attempt?. I’m not sure and I don’t know but the video was released after.I used the Blue Origin approach last year,when I tried to work on 12 different projects last year. As Ialready mentioned, didn’t work well.So this year,I’m switching it up to the SpaceX approach. So this is me saying I’ll be working on12 new personal projects,one for each month for the year 2016. Some of them will be opensourcesome may not. But every project will be accompanied with a blog post about how it works and how itwas developed. I might work with other devs on some as well.I’m looking forward to 2016. Hoping for interesting projects,very good solutions and an awesome 2016. Happy 2016. Stay winning." }, { "title": "Design Thinking Approach For Hobby Robotics Project", "url": "/posts/Design-Thinking-Approach-For-Hobby-Robotic-Project/", "categories": "electronics", "tags": "documentation, sample", "date": "2015-12-21 00:00:00 +0100", "snippet": "I work at DreamOval and we were privileged to be part of the Stanford Seedtraining this year. One of the highlights of the training was Design Thinking.What is design thinking?.(follow link)I wanted to apply design thinking to a hobby project. A hardware one. It shouldn’t be too simple. This automatically takes out the hello world of hardware projects,blinking LED, out of this.Definitely not a mars rover too. So this is an in-between kinda project. But closer to the blinkingLED.We are going to design,code,build a simple robot. But let me add that the real focus of this is not the project itself. But the processes involved in getting this done.It would be a 4-part (maybe 5) series. This is the first part of the series.Personally when it comes to my robotic projects. I like to see what I plan to have in the end beforeI start. Some people prefer to design their circuits before knowing where to place each component thatmakes their build. But having a design in mind helps me know what components to choose.An example is, if after design I realise I need to build a robot where its size matter during thedesign process I’ll rather I pick ATmega328 mcu compared to an arduino.The design process is the most difficult part for me. Why? because this is the part you see the endfrom the beginning. Most hobby project tend to be open ended because we get carried away and run wild with features and upgrades. Using design thinking approach helps in solving this.1. EmpathyEmpathy involves us sharing the feeling of another.This makes this part a bit tricky because we areabout to share our feeling with ourselves. If you are working in groups you can pick partners.After interviewing myself. This is what I gathered. Its ok if your results vary.* This is a hobby.* Budget is very small.* Robot should be able to move and see its environment.* User (I) should be able to see what the robot sees.* I should be able to control the robot.* Robot should be able to move on its own.* Again, budget is really small.* Robot should move around easily on its own in a known or unknown environment.Thats about it. Lets move on to the next step.2. DefineNow using our interview results we can define what the user wants. We should be able to see the user’sreal problem. Defining it brings us a step closer to solving the real problem for the user.This can be iterative process.* Robot should use relatively cheap components.* Robot should be able to move on its own.* Robot should move by acting on commands from user.* User should \"see\" what robot \"sees\".* Robot should not be huge.* Robot should be able to \"think\" on its own.3. IdeateThis is where we find possible solutions to the problem. Building up on what we gathered from defining theproblems/solutions.Based on what we defined I was able to create 5 categories. The one section I didn’t gather from interviewing myself is power source,I added that. Its not always necessary to list out categories.List out all the ideas that come to mind here for creating the solution. Then strike out the ones thatwon’t work. Power Source 5V battery that can be changed. Solar ? Sense & Sight Camera for sight UltraSonic Sensor would be used (just like bats) Send realtime stream from camera over Wi-Fi to user Identify objects by \"remembering\" them Tag objects by shape and/or color Movement Wheels for movement Tank treads for movement Metal bended as legs Fly-- mini drone/quad or tri -copter Interaction Ultrasonic sensor to measure distance from objects Can move objects in its path Send relative position of objects to be mapped Should be able to jump or fly over objects in its path Intelligence Should be able to remember paths already taken to pick best in terms of distance on another occasion Identify objects by \"remembering\" them Control Send and receive signals via Wi-Fi - via Bluetooth - via ? After striking out the ones not necessary based on requirements gatheredunder the sections, Empathy and Define.I had to remove most of the ideas because I wanted to keep the budget really small.The best approach however is to prototype each of the ideas preferably in groups.4. PrototypeUsing generated ideas we can build examples of the robots product and explain how it works.I use SketchUp for building my prototypes. Use whatever you are comfortable with. If you have a 3D printer use it. Unfortunately I don’t. Next up we’ll continue working on the prototype section. Perfect design in SketchUp. Pick electronic components,design and program test robot. Simulate build and test robot in an environment. Putting it together,build and test. Part 2 " }, { "title": "Multiple Spring Data MongoDB Connections", "url": "/posts/Mulitple-Spring-Data-MongoDB-Connections/", "categories": "", "tags": "documentation, sample", "date": "2015-10-09 00:00:00 +0200", "snippet": "Using Spring Data Starter MongoDB to connect to multiple MongoDB databases is quite easy. Its less complicated whenyou use just MongoTemplate for all your queries. But gets complicated when you use both MongoTemplate and MongoRepository. MongoTemplate connects to the intended secondary datasource but MongoRepository still uses the primary(strange but true).Fortunately there is a simple way to solve it.1. Create your custom MongoTemplate bean for your secondary datasource.@Beanpublic Mongo mongo() throws Exception{ return new MongoClient(host,port);}@Bean(autowire = Autowire.BY_NAME, name = \"secondaryMongoTemplate\")public MongoTemplate secondaryMongoTemplate() throws Exception {\treturn new MongoTemplate(mongo(),database);}2. And Finally @EnableMongoRepositories with the custom MongoTemplate@EnableMongoRepositories( basePackages ={\"this.is.your.repository.package\"}, mongoTemplateRef = \"secondaryMongoTemplate\" )Now all MongoRepository implementations would use the same datasource configuration as your custom MongoTemplate. Ok. Later." }, { "title": "Spring Metrics For Simple Event Monitoring", "url": "/posts/Spring-Metrics/", "categories": "tech", "tags": "documentation, sample", "date": "2015-08-25 00:00:00 +0200", "snippet": "Sometimes (or all the time) you would want to know whats happening in your application’s life cycle.Spring Boot Metrics is just the right toolfor application metrics in any Spring Boot Application. It exposes 2 metrics services, “Gauge” , “Counter” out of the box as well as an interface PublicMetrics to capture custommetrics.We would use all 3 to capture application level metrics.Before getting this to work add the Spring Boot Actuator Dependency (I assume you are using maven) &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;version&gt;{version}&lt;/version&gt; &lt;/dependency&gt;The first two are fairly simple. A simple 2 step process of initialize and use.1. GaugeA Gauge helps record a single value. Data type is double.Autowiring GaugeService before using it.@Autowiredprivate GaugeService gaugeService;thengaugeService.submit(\"sample.metric\", 20); //20 is sample metric value2. CounterA Counter records an increment or decrement. Data type is int.@Autowiredprivate CounterService counterService;thiscounterService.increment(\"sample.metric\"); //adds 1or thatcounterService.decrement(\"sample.metric\"); //subtracts 13. PublicMetricsThis uses a different approach. Implement PublicMetrics.then@Componentpublic class CustomMetric implements PublicMetrics { public CustomMetric() { } @Override public Collection&lt; Metric&lt; ?&gt; &gt; metrics() { Collection&lt; Metric&lt; ?&gt; &gt; result = new LinkedHashSet&lt;&gt;(); result.add(new Metric&lt;&gt;(\"sample.metric\", 20)); //20 is sample metric value // u can add multiple metrics to results return result; }}Now that we’ve captured the metrics. How do we read it?Goto http://localhost:{port}/metrics. That’s about it." }, { "title": "Using Spring Security OAuth 2.0 and MongoDB to secure a Microservice/SOA System", "url": "/posts/Spring-Security-OAuth2/", "categories": "distributed", "tags": "documentation, sample", "date": "2015-08-24 00:00:00 +0200", "snippet": "Before we go straight to the how-to and codes. I’d like to take a minute to explainmy choice in using Spring Security OAuth2.0 and MongoDB to develop a Single Sign On Authentication Server.With every microservice architecture,there would obviously different services working together.Unlike the macro or fat -war or fat -jar architecture, managing authentication and authorization is not the same.Because assuming you have two microservice working together to delivery a service. It is would be a bad idea to replicateauthentication and authorization for both microservices independently.Now that we are both on the same page that replicating authentication and authorization for all microservices is really ..erm.We both know we need to have a centralized authentication system where a user authenticates andis authorized on one system, her privileges are recognized throughout all other microservices in the architecture.Let’s talk a bit about RESTful services, Stateless Sessions and Stateful Sessions.To access any content on a server 3 things are required to be done Authentication - Verify user identity Authorization - Is she supposed to have access to that? Session Management - I know who you are and you have access for this timeIn any RESTful system, Number 3. is not encouraged. Why? It would mean that to get things working we would need to authenticate the user for every RESTful request.This is not good. Assuming I have a huge user database it would be worse.The best next fit is to give the user a token once authenticated and this can be used for subsequent requests.Token would be managed separately from the users. Expired tokens deleted(To keep token storage small). This means that on a less busy day we would have just 1 token stored and (user size ) == token size on our busiest day. Compared to havingsame (user size) for the busiest and less busy days,this is much better.But in THEORY, RESTful systems should be STATELESS.What’s Stateless and Stateful Sessions?STATELESS SESSIONS“In computing, a stateless protocol is a communications protocol that treats each request as an independent transaction that is unrelated to any previous request so that the communication consists of independent pairs of request and response. A stateless protocol does not require the server to retain session information or status about each communications partner for the duration of multiple requests. In contrast, a protocol which requires keeping of the internal state on the server is known as a stateful protocol.”STATEFUL SESSIONSExact opposite of Stateless.But practically it would be impossible to use a “pure” stateless session management for RESTful services by the RESTful standard without having performance issues.For a frontend applications(eg: AngularJS or Polymer) once it does not keep tokens in cookies,to prevent Cross Site Request Forgery [CSRF or XSRF].And with the rise of “MOBILE FIRST” systems,this would be a problem because mobile apps don’t play well with COOKIEs.For backend application storing the token would be quite easy. Since there are lots of options for this.MongoDB and Redis are verygood candidates.If the cookie approach is not “ok” for frontend applications how do we store tokens then?HTML5 comes with localStorage and sessionStorage.“With local storage, web applications can store data locally within the user’s browser.Before HTML5, application data had to be stored in cookies, included in every server request. Local storage is more secure, and large amounts of data can be stored locally, without affecting website performance.Unlike cookies, the storage limit is far larger (at least 5MB) and information is never transferred to the server.Local storage is per origin (per domain and protocol). All pages, from one origin, can store and access the same data.”The main difference between the two is…“The sessionStorage object is equal to the localStorage object, except that it stores the data for only one session”Although SessionStorage is not supported in many browsers. You can get localStorage working as sessionStorage with some extra js codes or just use polyfill to provide this functionality in unsupported browsers.You can then pick tokens from the sessionStorage (or localStorage ) and send as HTTP headers.Now that we have clear idea on how our Single Sign On Authentication Server should work,both backend and frontend(and a little about mobile) considered let me introduce you to OAuth2.0.Whats OAuth2.0?“The OAuth 2.0 authorization framework enables a third-party application to obtain limited access to an HTTP service, either on behalf of a resource owner by orchestrating an approval interaction between the resource owner and the HTTP service, or by allowing the third-party application to obtain access on its own behalf.”Fortunately it supports what we want our authentication server should achieve (plus more features)A quick search online would let you know Google,Facebook,Windows Live,GitHub,Slack,Dropbox and many more are already using it.They using it doesn’t make it perfect,it just means we are using them as guinea pigs. :D.Short Summary about OAuth 2.0“The first step of OAuth 2 is to get authorization from the user. For browser-based or mobile apps, this is usually accomplished by displaying an interface provided by the service to the user.OAuth 2 provides several “grant types” for different use cases. The grant types defined are: Authorization Code for apps running on a web server Implicit for browser-based or mobile apps Password for logging in with a username and password Client credentials for application accessWith these grant types we can authenticate and authorize users as well clients(which is microservice in our case).I have to add this disclaimer here. For every framework,new technology whatevs I have/want to use. Spring is my first go to client or integration.I mean it’s Spring.Why Spring Security OAuth2.0 ?Spring already has integration and support for LDAP(easy integration if your Single Sign On Authentication Server is an enterprise solution),Social platforms and Databases.The real question isWHY NOT Spring Security OAuth2.0?.Issues with Spring Security OAuth2.0Spring Security OAuth 2.0 supports storing tokens in MySQL out of the box.With the abundance of NoSQL databases which Spring already supports it would be a better option to integrate with one of them out of the box.But let me mention here that Spring Security OAuth 2.0 supports InMemoryTokenStore and JWT as well.“The default InMemoryTokenStore is perfectly fine for a single server (i.e. low traffic and no hot swap to a backup server in the case of failure). Most projects can start here, and maybe operate this way in development mode, to make it easy to start a server with no dependencies.The JdbcTokenStore is the JDBC version of the same thing, which stores token data in a relational database. Use the JDBC version if you can share a database between servers, either scaled up instances of the same server if there is only one, or the Authorization and Resources Servers if there are multiple components. To use the JdbcTokenStore you need “spring-jdbc” on the classpath.The JSON Web Token (JWT) version of the store encodes all the data about the grant into the token itself (so no back end store at all which is a significant advantage). One disadvantage is that you can’t easily revoke an access token, so they normally are granted with short expiry and the revocation is handled at the refresh token. Another disadvantage is that the tokens can get quite large if you are storing a lot of user credential information in them. The JwtTokenStore is not really a “store” in the sense that it doesn’t persist any data, but it plays the same role of translating between token values and authentication information in the DefaultTokenServices.”But the good thing is you can implement the TokenStore class and persist your token in any database of your choice.I chose MongoDB. I need to mention that were some issues with the MongoDB converter using this approach but it finally worked :)UpdateThere are some issues with the project,if you use it as it is you would have problems with the antMatchersThey do not work when you have custom urls to be permitted. For example :If I wanted this url http://localhost:8080/donotauthenticate to be permitted with code below http .authorizeRequests() .antMatchers(\"/donotauthenticate\").permitAll() .antMatchers(\"/**\").authenticated()You’ll see it doesn’t work when you using this, which I did in my case &lt;dependency&gt;\t &lt;groupId&gt;org.springframework.security.oauth&lt;/groupId&gt;\t &lt;artifactId&gt;spring-security-oauth2&lt;/artifactId&gt;\t &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;/dependency&gt;Changing it to this version fixed the issue. &lt;dependency&gt; \t&lt;groupId&gt;org.springframework.security.oauth&lt;/groupId&gt;\t&lt;artifactId&gt;spring-security-oauth2&lt;/artifactId&gt;\t&lt;version&gt;2.0.7.RELEASE&lt;/version&gt; &lt;/dependency&gt;I’ve not looked into exactly what the cause is to file an issue. Anyone who does before me can comment Now that we done talking about our choices. Lets move on to coding this.Referenceshttps://speakerdeck.com/dsyer/security-for-microservices-with-spring - Dave Syerhttps://aaronparecki.com/articles/2012/07/29/1/oauth2-simplified - Aaron Pareckihttp://www.w3schools.com/html/html5_webstorage.asp" }, { "title": "Lambda Architecture With Kafka, ElasticSearch, Apache Storm and MongoDB", "url": "/posts/Lambda-Architecture-RT/", "categories": "distributed", "tags": "documentation, sample", "date": "2015-08-23 00:00:00 +0200", "snippet": "How I would use Apache Storm,Apache Kafka,Elasticsearch and MongoDB for a monitoring system based on the lambda architecture.What is Lambda Architecture?It’s a design principle where all derived calculations in a data system can be expressed as a re-computation function over all of your data. This re-computation would be done over immutable data readily available.Lambda Architecture is made of 3 layers,part played in the Lambda Architecture is summarized below : Batch Layer which computes functions over all data with high latency and rewrites immutable fact transformations into your data stores (e.g. via Hadoop,MongoDB in our case) Speed Layer which computes functions over recent data with low latency and mutates your real-time data stores directly (e.g. via Storm) Serving Layer which abstracts over those two to provide unified answers to real-time and historical questions (e.g. Cloudera Impala,ElasticSearch in our case) Batch Layer and Why use MongoDB?The purpose of the batch layer is to serve as the immutable data store. Its purpose is to read and create out of the the fourCRUD(Create,Read,Update and Delete) operations. (Reason the data is immutable). The database to be used for the batch layershould be able to support faster reads and write. With these as our priority we can safely cross out relational databasesbecause consistency is not our highest priority when it comes to reading and creating.Because I don’t have the time to test all the available NoSQL databases to see which of them supports faster reads and writes,I searched online. I regretted. To be honest,this was the hardest part. I was so overwhelmed with contradicting performance reports I felt I should test them all myself. But I don’t have the time or the resources to do that now.Hands down,the clear winner is Hadoop. I was actually looking for Hadoop alternative because I wanted anyone could easilydeploy and have it up and running in no time.So I picked MongoDB.Disclaimer: I picked MongoDB as my batch layer,it doesn’t in anyway mean its the best option(at the time of this post). So If you or your organization has a better reason to pick a MongoDB alternative. Please do.Other considerations were Aerospike, Cassandra,ElephantDB and definitely Hadoop.But whatever your choice is,the goal is to have faster,concurrent reads and writes. Speed Layer and Why use Apache Storm and Apache Kafka?The speed layer or the real time layer takes the stream of data,real time,calculates indexes the data. Obviously this is not immutable data.Why do I use Apache Storm? It was “born” for this.“Apache Storm is a free and open source distributed realtime computation system. Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing. Storm is simple, can be used with any programming language, and is a lot of fun to use!Storm has many use cases: realtime analytics, online machine learning, continuous computation, distributed RPC, ETL, and more. Storm is fast: a benchmark clocked it at over a million tuples processed per second per node. It is scalable, fault-tolerant, guarantees your data will be processed, and is easy to set up and operate.”And also Nathan Marz who first wrote about the Lambda Architecture created Apache Storm as well.Why Apache Kafka?We need a resilient messaging queue that would feed the speed layer with the stream of data. Sort of like a pool forall stream data.So we have one source for getting the data.This and this is why I chose Apache Kafka. Serving Layer and Why use ElasticSearch?“Elasticsearch is a flexible and powerful open source, distributed, real-time search and analytics engine. Architecture is from the ground up for use in distributed environments where reliability and scalability are must haves, Elasticsearch gives you the ability to move easily beyond simple full-text search. Through its robust set of APIs and query DSLs, plus clients for the most popular programming languages, Elasticsearch delivers on the near limitless promises of search technology”The serving layer combines the output from the batch and speed layer. This layer helps us get the data,combined, fromboth the batch and serving layer.Output from the batch and speed layers are stored in the serving layer, which responds to ad-hoc queries by returning pre-computed views or building views from the processed dataThe serving layer provides and answers to getting historical(batch) data and real time data(speed).If you end up using MongoDB as your batch layer you can use MongoDB River. This would help keep (backup) records for the historical data with minimal configuration.For the real-time data.Our Speed layer using a Storm Spout would help us aggregate and index real-time data in ElasticSearch. Putting It All together - The Lambda RT ProjectREFERENCEShttp://jameskinley.tumblr.com/post/37398560534/the-lambda-architecture-principles-for-architecting - James Kinley" }, { "title": "Java Annotations + Spring = Awesome", "url": "/posts/Annotations+Spring=Awesome/", "categories": "tech", "tags": "documentation, sample", "date": "2015-08-22 00:00:00 +0200", "snippet": "Java Annotations are really cool. They give this feature to send extra information that can be used during runtimeand/or compile time.I don’t need to talk about Spring here. I personally think it the best thing to happen to JEE.Combining Java Annotations and Spring IOC we can get this “clean” thing we would learn about in the next 3 min.Ok. Here is the summary of what I wanted to do and how I achieved it.What to do? Design a framework that can easily be integrated to read data from multiple sources and then index it in a single destination .(This is a sample scenario)How I did it?1. Created annotation Foo @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.TYPE) public @interface FooAnnotation { }2. Created interfaceThis interface is to serve as a “standard” for all systems who want to index their data.public interface Bar&lt; T&gt; { public List&lt; T&gt; getAllBar();}3. Implement All the BARs you canWe use @Component so that the Spring IOC component scan can pick it up@Component@FooAnnotationpublic class BarI implements Bar&lt; BarIData&gt;{\t@Override public List&lt; BarIData&gt; getAllBar(){ \t// lets get BarIData for this implementation here }}4. Get All BARs Implementation with FooAnnotationNow this where Spring IOC comes in. Once you have your ApplicationContext autowired.@Autowiredprivate ApplicationContext context;You can get all beans with FooAnnotation// get my beansMap&lt;String, Object&gt; myBeans = context.getBeansWithAnnotation(FooAnnotation.class);myBeans.keySet().stream().forEach((b) -&gt; {Bar&lt; Object&gt; bar = (Bar) context.getBean(b);//use this implementation of bar however you wantbar.getAllBar(); }); Yep.That should work." } ]
